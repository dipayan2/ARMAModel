{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "# from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.graphics.tsaplots import plot_predict\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skfda\n",
    "from skfda.ml.clustering import FuzzyCMeans, KMeans\n",
    "class OSLFuzzyCluster:\n",
    "    def __init__(self,data,n_cluster=3,init_cluster=None,fuzzifier=2):\n",
    "        self.data = data\n",
    "        self.nclust = n_cluster\n",
    "        self.init_clust = init_cluster\n",
    "        self.model = None\n",
    "        self.predictClus = None\n",
    "        self.fData = None\n",
    "        self.fuzzifier=fuzzifier\n",
    "        self.probCond = None\n",
    "    \n",
    "    def getFDataGrid(self, inp):\n",
    "        inp = np.array(inp)\n",
    "        n_dim = inp.ndim\n",
    "        if n_dim == 1:\n",
    "            grid_point = list(range(len(inp)))\n",
    "            data_grid = []\n",
    "            data_grid.append(inp)\n",
    "            fd = skfda.FDataGrid(data_grid,grid_point)\n",
    "            return fd\n",
    "        elif n_dim == 2:\n",
    "            grid_point = list(range(len(inp[0])))\n",
    "            data_grid = []\n",
    "            for i in range(len(inp)):\n",
    "                data_grid.append(inp[i])\n",
    "\n",
    "            fd = skfda.FDataGrid(data_grid,grid_point)\n",
    "            return fd\n",
    "        else:\n",
    "            raise Exception(\"Cannot handle more than 2 dimension for now\") \n",
    "            return None\n",
    "    \n",
    "    def update_data(self,data):\n",
    "        self.data = np.append(self.data,np.array(data))\n",
    "        return\n",
    "        \n",
    "        \n",
    "    def fit(self):    \n",
    "        self.fData = self.getFDataGrid(self.data)\n",
    "        if self.fData == None:\n",
    "            print(\"No data to cluster\")\n",
    "            return None\n",
    "        self.model = FuzzyCMeans(n_clusters=self.nclust,n_init=5 ,init=self.init_clust,fuzzifier=self.fuzzifier,max_iter=400,random_state=0)\n",
    "        self.model.fit(self.fData)\n",
    "        self.predictClus = self.model.predict_proba(self.fData)\n",
    "        return self.model,self.predictClus\n",
    "    \n",
    "    def predict_clust(self,data):\n",
    "        fd = self.getFDataGrid(data)\n",
    "        return self.model.predict_proba(fd)\n",
    "    \n",
    "    def get_center(self):\n",
    "        return self.model.cluster_centers_.data_matrix\n",
    "        \n",
    "    def get_CondProb(self):\n",
    "        prob_count = self.predictClus\n",
    "        n_cluster = self.nclust\n",
    "\n",
    "        PCount = [0 for _ in range(n_cluster)]\n",
    "        PTP_count = [[0 for _ in range(n_cluster)] for _ in range(n_cluster)]\n",
    "        for i in range(len(prob_count)):\n",
    "            for c in range(n_cluster):\n",
    "                PCount[c] += prob_count[i][c]\n",
    "                if i>0:\n",
    "                    for cn in range(n_cluster):\n",
    "                        PTP_count[cn][c] += prob_count[i-1][cn]*prob_count[i][c]\n",
    "\n",
    "\n",
    "        self.probCond = [[0 for _ in range(n_cluster)] for _ in range(n_cluster)]\n",
    "        for i in range(n_cluster):\n",
    "            for j in range(n_cluster):\n",
    "                self.probCond[i][j] = round(PTP_count[i][j]/PCount[i],3) # P(Y(t) in j / Y(t-1) in i)\n",
    "        return self.probCond\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OSLARMA:\n",
    "    def __init__(self,endog,p=3,exog=None,weight=None):\n",
    "        self.endog = endog\n",
    "        self.p = p\n",
    "        self.exog = exog\n",
    "        self.w = weight\n",
    "        self.model = None\n",
    "        self.xData = None\n",
    "        self.yData = None\n",
    "        self.ylast = None\n",
    "        self.fitData = None\n",
    "    \n",
    "    def summary(self):\n",
    "        return self.model.summary()\n",
    "        \n",
    "    def getShifted_Dataset(self):\n",
    "        # Currently accepting only one dimensional exog if any\n",
    "        if self.p < 1:\n",
    "            print('Error needs atleast 1')\n",
    "            return None\n",
    "        plen = self.p\n",
    "        df_temp = pd.DataFrame()\n",
    "        df_temp['endog'] = self.endog\n",
    "        # Getting shifted dataset\n",
    "        for i in range(1,self.p+1):\n",
    "            df_temp['Shift_endog_%d' % i] = df_temp['endog'].shift(i)\n",
    "        if self.exog is not None:\n",
    "            df_temp['exog'] = self.exog\n",
    "            plen += 1\n",
    "        if self.w is not None:\n",
    "            df_temp['Weight'] = self.w\n",
    "        elif self.w is None:\n",
    "            df_temp['Weight'] = np.ones(self.endog.shape[0])\n",
    "\n",
    "        df_train_2 = df_temp.dropna()\n",
    "        X_train = df_train_2.iloc[:,1:plen+1].values.reshape(-1,plen)\n",
    "        y_train = df_train_2.iloc[:,0].values.reshape(-1,1)\n",
    "        sample_weight = df_train_2['Weight'].values\n",
    "        X_last = df_train_2.iloc[-1,:self.p].values.reshape(-1,self.p)\n",
    "        return [y_train,X_train,X_last[0],sample_weight]\n",
    "    \n",
    "    def update_data(self,endog,exog=None,weight=None):\n",
    "        # Addpending the data\n",
    "        self.endog = np.concatenate((self.endog,np.array(endog)))\n",
    "        \n",
    "        if exog is not None:\n",
    "            self.exog = np.concatenate((self.exog, np.array(exog)))\n",
    "        if weight is not None:\n",
    "            self.w = np.concatenate((self.w,np.array(weight)))\n",
    "        # This part is helps adding the new observed data and can be used to retrain the model\n",
    "        self.ylast = self.endog[-1:-(self.p+1):-1]\n",
    "#         #Test Code\n",
    "#         print(\"Update\",self.ylast)\n",
    "#         #End testcode\n",
    "        return\n",
    "    \n",
    "    def WeighedLearn(self,y,X,weight):\n",
    "        import statsmodels.api as sm\n",
    "        X_sm = sm.add_constant(X)\n",
    "        model = sm.WLS(y,X_sm,weights=weight)\n",
    "        result = model.fit()\n",
    "        return result\n",
    "    \n",
    "    def fit(self):\n",
    "        tData = self.getShifted_Dataset()\n",
    "#         self.yData = tData[0]\n",
    "#         self.xData = tData[1]\n",
    "        self.ylast = tData[2]\n",
    "#         weight = tData[3]\n",
    "        self.model = self.WeighedLearn(tData[0],tData[1],weight=tData[3])\n",
    "        return self.model,self.ylast\n",
    "           \n",
    "    def get_predict(self,steps=1,exog=None):\n",
    "        # Ensure that the exog and model have the same dimension. Currently we have only one dim of exog\n",
    "        if exog is not None:\n",
    "            #Update\n",
    "            if len(exog) != steps:\n",
    "                print(\"Exog variable incorrect\")\n",
    "                return None\n",
    "        # Assume exog is there\n",
    "        Y_out = []\n",
    "#         print(\"exog\",exog)\n",
    "        if exog is not None:\n",
    "            X_update = []        \n",
    "            X_inp = self.ylast\n",
    "#             print(\"ylast\",self.ylast)\n",
    "            for i in range(steps):\n",
    "                X_test = [1.0]\n",
    "                X_test.extend(X_inp)\n",
    "                X_test.append(exog[i])\n",
    "#                 print(\"Xtest\",X_test)\n",
    "                y_pred = self.model.get_prediction(X_test).predicted_mean\n",
    "                Y_out.extend(y_pred)\n",
    "                X_i = list(y_pred)\n",
    "                X_i.extend(X_inp[:-1])\n",
    "                X_inp = X_i\n",
    "#                 print(X_inp)\n",
    "            # Predict the data\n",
    "            return Y_out\n",
    "        else:\n",
    "            #No exog\n",
    "            X_inp = self.ylast\n",
    "            for i in range(steps):\n",
    "                X_test = [1.0]\n",
    "                X_test.extend(X_inp)\n",
    "                y_pred = self.model.get_prediction(X_test).predicted_mean\n",
    "                Y_out.extend(y_pred)\n",
    "                X_i = list(y_pred)\n",
    "                X_i.extend(X_inp[:-1])\n",
    "                X_inp = X_i\n",
    "    #             print(X_inp)\n",
    "            return Y_out        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Modifier Code\n",
    "def getDataFrame(fileName):\n",
    "    myDf = pd.read_csv(fileName)\n",
    "    myDf['memLoad'] = myDf['memUse']*100/myDf['memTot']\n",
    "    myDf['memScore'] = myDf['memLoad']*myDf['memfreq']/825000000\n",
    "    myDf['memF'] = myDf['memfreq']*100/825000000\n",
    "    myDf['cpuScore'] = myDf['cpuload']*myDf['cpufreq']/1400000\n",
    "    myDf['cpuF'] = myDf['cpufreq']*100/1000000\n",
    "    return myDf\n",
    "\n",
    "def getNewDataFrame(fileName):\n",
    "    myDf = pd.read_csv(fileName)\n",
    "#     myDf['memLoad'] = myDf['memUse']*100/myDf['memTot']\n",
    "    myDf['mUtil'] = myDf['mUtil'].clip(upper=100)\n",
    "    myDf['mScore'] = myDf['mUtil']*myDf['mFreq']/825000000\n",
    "    myDf['memF'] = myDf['mFreq']*100/825000000\n",
    "    myDf['bcScore'] = myDf['bUtil']*myDf['bFreq']/1400000\n",
    "#     myDf['cpuF'] = myDf['cpufreq']*100/1000000\n",
    "    return myDf\n",
    "\n",
    "def getDataMatrix(dx,dy=None):\n",
    "    data_matrix=[]\n",
    "    if dy is None:\n",
    "        for v in dx:\n",
    "            data_matrix.append([round(v,2),round(v,2)])\n",
    "    else:\n",
    "        for v in zip(dx,dy):\n",
    "            data_matrix.append([round(v[0],2),round(v[1],2)])\n",
    "    return data_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latest Code\n",
    "def main():\n",
    "    utilData = getNewDataFrame('./UtilTestFinal')\n",
    "    print(\"Size of data:\",utilData.shape)\n",
    "    dy = utilData.bUtil.dropna().to_numpy()\n",
    "    dz = utilData.mUtil.dropna().to_numpy()\n",
    "#     dy = utilData.cpuload.dropna().to_numpy()\n",
    "    nCluster = 3\n",
    "    y_train = dy[1:6000]\n",
    "    x_train = dz[:5999]\n",
    "    # Predicting in a different range\n",
    "    y_test = dy[7000:]\n",
    "    x_test = dz[6999:]\n",
    "    y_dmat = getDataMatrix(y_train,x_train)\n",
    "    myClust = OSLFuzzyCluster(y_dmat,n_cluster=nCluster)\n",
    "    _, data_weight = myClust.fit()\n",
    "#     print(dy[0])\n",
    "#     print(data_weight)\n",
    "#     print(\"data_cluster\", myClust.get_center())\n",
    "    trans_prob = myClust.get_CondProb()\n",
    "    #Removing Print\n",
    "#     print(trans_prob)\n",
    "#     print(\"\\n\\n\\n\")\n",
    "    # We will send the weights of the data and then train these models\n",
    "    models = []\n",
    "    # Train the models\n",
    "    for i in range(nCluster):\n",
    "        model = OSLARMA(y_train,p=4,exog=x_train, weight=data_weight[:,i])\n",
    "        model.fit()\n",
    "#         print(model.summary())\n",
    "        models.append(model)\n",
    "    \n",
    "    # Models are trained now begin prediction\n",
    "#     print(\"last actual value\", y_train[-1],\"Likelihood cluster\", myClust.predict_clust([ y_train[-1], y_train[-1]]))\n",
    "    y_prior = dy[6999]\n",
    "    x_prior = dz[6998]\n",
    "    for i in range(50):\n",
    "        # Get prob of P(Y(t-1) in i) using cluster\n",
    "        P_prior = myClust.predict_clust([y_prior,x_prior])[0]\n",
    "#         print(\"P\",P_prior)\n",
    "        P_like = []\n",
    "        # Predict likelihood of Y(t) in cluster based on transition prob\n",
    "        for k in range(nCluster):\n",
    "            prob = 0\n",
    "            for j in range(nCluster):\n",
    "                prob += trans_prob[j][k]*P_prior[j]\n",
    "            P_like.append(prob)\n",
    "        \n",
    "#         print(\"PLike\",P_like)\n",
    "        y_act = y_test[i]\n",
    "        y_prior = y_act\n",
    "        x_prior = dz[6999+i]\n",
    "        y_pred = []\n",
    "        y_est = 0\n",
    "        for j in range(nCluster):\n",
    "            y_pred.append(models[j].get_predict(1,[x_prior])[0])\n",
    "            y_est += y_pred[-1]*P_like[j]\n",
    "        \n",
    "        print(i, y_act,y_est,x_prior)\n",
    "        \n",
    "        for j in range(nCluster):\n",
    "            models[j].update_data([y_act],[x_prior])\n",
    "            \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of data: (8560, 10)\n",
      "0 100 45.56968790439381 1\n",
      "1 100 75.66413558332208 1\n",
      "2 100 85.39948981619987 2\n",
      "3 100 90.00471677686707 2\n",
      "4 100 87.31173766123024 1\n",
      "5 100 87.00614096766205 2\n",
      "6 100 87.31173766123024 1\n",
      "7 100 87.21527734472983 1\n",
      "8 100 87.00614096766205 2\n",
      "9 100 87.31173766123024 1\n",
      "10 100 87.21527734472983 1\n",
      "11 100 87.00614096766205 2\n",
      "12 100 87.10169221802917 2\n",
      "13 100 87.31173766123024 1\n",
      "14 100 87.00614096766205 2\n",
      "15 100 87.10169221802917 2\n",
      "16 100 87.31173766123024 1\n",
      "17 100 87.00614096766205 2\n",
      "18 100 87.10169221802917 2\n",
      "19 100 87.31173766123024 1\n",
      "20 100 87.00614096766205 2\n",
      "21 100 87.31173766123024 1\n",
      "22 100 87.21527734472983 1\n",
      "23 100 87.00614096766205 2\n",
      "24 100 87.10169221802917 2\n",
      "25 100 87.31173766123024 1\n",
      "26 100 87.00614096766205 2\n",
      "27 100 87.10169221802917 2\n",
      "28 100 87.31173766123024 1\n",
      "29 100 87.00614096766205 2\n",
      "30 100 87.10169221802917 2\n",
      "31 100 87.10169221802917 2\n",
      "32 100 87.31173766123024 1\n",
      "33 100 87.21527734472983 1\n",
      "34 100 87.00614096766205 2\n",
      "35 100 87.31173766123024 1\n",
      "36 100 87.21527734472983 1\n",
      "37 100 86.7970045905943 3\n",
      "38 100 86.96071589804629 3\n",
      "39 100 87.38213349709092 1\n",
      "40 100 87.00614096766205 2\n",
      "41 100 87.10169221802917 2\n",
      "42 100 87.31173766123024 1\n",
      "43 100 87.00614096766205 2\n",
      "44 100 87.10169221802917 2\n",
      "45 100 87.10169221802917 2\n",
      "46 100 87.10169221802917 2\n",
      "47 100 87.10169221802917 2\n",
      "48 100 87.10169221802917 2\n",
      "49 100 87.10169221802917 2\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old Code\n",
    "# Testing without exog\n",
    "def main():\n",
    "    utilData = getDataFrame('./Util_Run1.csv')\n",
    "    dy = utilData.cpuload.dropna().to_numpy()\n",
    "    dz = utilData.memLoad.dropna().to_numpy()\n",
    "#     dy = utilData.cpuload.dropna().to_numpy()\n",
    "    nCluster = 5\n",
    "    y_train = dy[:2000]\n",
    "    x_train = dz [:2000]\n",
    "    y_test = dy[2000:]\n",
    "    x_test = dz[2000:]\n",
    "    y_dmat = getDataMatrix(y_train,x_train)\n",
    "    myClust = OSLFuzzyCluster(y_dmat,n_cluster=nCluster)\n",
    "    _, data_weight = myClust.fit()\n",
    "#     print(dy[0])\n",
    "#     print(data_weight)\n",
    "#     print(\"data_cluster\", myClust.get_center())\n",
    "    trans_prob = myClust.get_CondProb()\n",
    "    print(trans_prob)\n",
    "    print(\"\\n\\n\\n\")\n",
    "    # We will send the weights of the data and then train these models\n",
    "    models = []\n",
    "    # Train the models\n",
    "    for i in range(nCluster):\n",
    "        model = OSLARMA(y_train,p=4,exog=x_train, weight=data_weight[:,i])\n",
    "        model.fit()\n",
    "#         print(model.summary())\n",
    "        models.append(model)\n",
    "    \n",
    "    # Models are trained now begin prediction\n",
    "#     print(\"last actual value\", y_train[-1],\"Likelihood cluster\", myClust.predict_clust([ y_train[-1], y_train[-1]]))\n",
    "    y_prior = y_train[-1]\n",
    "    x_prior = x_train[-1]\n",
    "    for i in range(50):\n",
    "        # Get prob of P(Y(t-1) in i) using cluster\n",
    "        P_prior = myClust.predict_clust([y_prior,x_prior])[0]\n",
    "#         print(\"P\",P_prior)\n",
    "        P_like = []\n",
    "        # Predict likelihood of Y(t) in cluster based on transition prob\n",
    "        for k in range(nCluster):\n",
    "            prob = 0\n",
    "            for j in range(nCluster):\n",
    "                prob += trans_prob[j][k]*P_prior[j]\n",
    "            P_like.append(prob)\n",
    "        \n",
    "#         print(\"PLike\",P_like)\n",
    "        y_act = y_test[i]\n",
    "        y_prior = y_act\n",
    "        x_prior = x_test[i]\n",
    "        y_pred = []\n",
    "        y_est = 0\n",
    "        for j in range(nCluster):\n",
    "            y_pred.append(models[j].get_predict(1,[x_prior])[0])\n",
    "            y_est += y_pred[-1]*P_like[j]\n",
    "        \n",
    "        print(i, y_act,y_est,x_prior)\n",
    "        \n",
    "        for j in range(nCluster):\n",
    "            models[j].update_data([y_act],[x_prior])\n",
    "            \n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
