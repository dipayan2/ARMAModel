{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "# from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.graphics.tsaplots import plot_predict\n",
    "import random\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "from scipy.signal import butter,filtfilt\n",
    "import numpy as np\n",
    "from scipy.signal import butter,filtfilt\n",
    "from statsmodels.graphics.tsaplots import plot_pacf, plot_acf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skfda\n",
    "from skfda.ml.clustering import FuzzyCMeans, KMeans\n",
    "class OSLFuzzyCluster:\n",
    "    '''\n",
    "    Class for fuzzy clustering \n",
    "    '''\n",
    "    def __init__(self,data,n_cluster=3,init_cluster=None,fuzzifier=2):\n",
    "        self.data = data\n",
    "        self.nclust = n_cluster\n",
    "        self.init_clust = init_cluster\n",
    "        self.model = None\n",
    "        self.predictClus = None\n",
    "        self.fData = None\n",
    "        self.fuzzifier=fuzzifier\n",
    "        self.probCond = None\n",
    "    \n",
    "    def getFDataGrid(self, inp):\n",
    "        '''\n",
    "        This function provides data transformation for the fuzzyCMeans package.\n",
    "        We provide a multi-dimensional list and transform to a FDataGrid datatype\n",
    "        '''\n",
    "        inp = np.array(inp)\n",
    "        n_dim = inp.ndim\n",
    "        if n_dim == 1:\n",
    "            grid_point = list(range(len(inp)))\n",
    "            data_grid = []\n",
    "            data_grid.append(inp)\n",
    "            fd = skfda.FDataGrid(data_grid,grid_point)\n",
    "            return fd\n",
    "        elif n_dim == 2:\n",
    "            grid_point = list(range(len(inp[0])))\n",
    "            data_grid = []\n",
    "            for i in range(len(inp)):\n",
    "                data_grid.append(inp[i])\n",
    "\n",
    "            fd = skfda.FDataGrid(data_grid,grid_point)\n",
    "            return fd\n",
    "        else:\n",
    "            raise Exception(\"Cannot handle more than 2 dimension for now\") \n",
    "            return None\n",
    "    \n",
    "    def update_data(self,data):\n",
    "        '''\n",
    "        This function allows to add more data to the object for future clustering \n",
    "        '''\n",
    "        self.data = np.append(self.data,np.array(data))\n",
    "        return\n",
    "        \n",
    "        \n",
    "    def fit(self):\n",
    "        '''\n",
    "        fit() function does two function:\n",
    "        1. Create a clustering model, which can provide the cluster probability of a data point\n",
    "        2. Provides predicted cluster probability of the trained data\n",
    "        '''\n",
    "        self.fData = self.getFDataGrid(self.data)\n",
    "        if self.fData == None:\n",
    "            print(\"No data to cluster\")\n",
    "            return None\n",
    "        self.model = FuzzyCMeans(n_clusters=self.nclust,n_init=5 ,init=self.init_clust,fuzzifier=self.fuzzifier,max_iter=400,random_state=0)\n",
    "        self.model.fit(self.fData)\n",
    "        self.predictClus = self.model.predict_proba(self.fData)\n",
    "        return self.model,self.predictClus\n",
    "    \n",
    "    def predict_clust(self,data):\n",
    "        '''\n",
    "        User function to get the cluster probability of a given data point\n",
    "        '''\n",
    "        fd = self.getFDataGrid(data)\n",
    "        return self.model.predict_proba(fd)\n",
    "    \n",
    "    def get_center(self):\n",
    "        '''\n",
    "        Allows us to get the cluster centers\n",
    "        '''\n",
    "        return self.model.cluster_centers_.data_matrix\n",
    "        \n",
    "    def get_CondProb(self):\n",
    "        '''\n",
    "        Provides the transitional probability among different clusters for the trained data\n",
    "        '''\n",
    "        prob_count = self.predictClus\n",
    "        n_cluster = self.nclust\n",
    "\n",
    "        PCount = [0 for _ in range(n_cluster)]\n",
    "        PTP_count = [[0 for _ in range(n_cluster)] for _ in range(n_cluster)]\n",
    "        for i in range(len(prob_count)):\n",
    "            for c in range(n_cluster):\n",
    "                PCount[c] += prob_count[i][c]\n",
    "                if i>0:\n",
    "                    for cn in range(n_cluster):\n",
    "                        PTP_count[cn][c] += prob_count[i-1][cn]*prob_count[i][c]\n",
    "\n",
    "\n",
    "        self.probCond = [[0 for _ in range(n_cluster)] for _ in range(n_cluster)]\n",
    "        for i in range(n_cluster):\n",
    "            for j in range(n_cluster):\n",
    "                self.probCond[i][j] = round(PTP_count[i][j]/PCount[i],3) # P(Y(t) in j / Y(t-1) in i)\n",
    "        return self.probCond\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "class OSLARMA:\n",
    "    def __init__(self,endog,p=3,q=0,exog=None,weight=None,shiftQ=5):\n",
    "#         import statsmodels.api as sm\n",
    "        self.endog = endog\n",
    "        self.p = p\n",
    "        self.q = q\n",
    "        self.ShQ = shiftQ\n",
    "        self.exog = np.array(exog)\n",
    "        self.w = weight # can use np.square or np.sqrt\n",
    "        self.model = None\n",
    "        self.MAmodel = None\n",
    "        self.xData = None\n",
    "        self.yData = None\n",
    "        self.ylast = None\n",
    "        self.fitData = None\n",
    "        self.residual = None\n",
    "        self.reslast = None\n",
    "    \n",
    "    def summary(self):\n",
    "        return self.model.summary()\n",
    "        \n",
    "    def getShifted_Dataset(self):\n",
    "        '''\n",
    "        Creates a shifted dataset for AR module, so we can transform a single dimensional time series\n",
    "        to a p-dimensional data for AR train and keep some data for the prediciting new data\n",
    "        '''\n",
    "        # Currently accepting only one dimensional exog if any\n",
    "        if self.p < 1:\n",
    "            print('Error needs atleast 1')\n",
    "            return None\n",
    "        plen = self.p\n",
    "        df_temp = pd.DataFrame()\n",
    "        df_temp['endog'] = self.endog\n",
    "        # Getting shifted dataset\n",
    "        for i in range(1,self.p+1):\n",
    "            df_temp['Shift_endog_%d' % i] = df_temp['endog'].shift(i)\n",
    "        if self.exog is not None:\n",
    "#             df_temp['exog'] = self.exog\n",
    "#             plen += 1\n",
    "#             # Shifting the exog variable\n",
    "            self.exog = np.array(self.exog)\n",
    "            for xid in range(self.exog.shape[0]):\n",
    "                df_temp['exog_%d' % xid] = self.exog[xid]\n",
    "                plen += 1\n",
    "        if self.w is not None:\n",
    "            df_temp['Weight'] = self.w\n",
    "        elif self.w is None:\n",
    "            df_temp['Weight'] = np.ones(self.endog.shape[0])\n",
    "\n",
    "        df_train_2 = df_temp.dropna()\n",
    "        X_train = df_train_2.iloc[:,1:plen+1].values.reshape(-1,plen)\n",
    "        y_train = df_train_2.iloc[:,0].values.reshape(-1,1)\n",
    "        sample_weight = df_train_2['Weight'].values\n",
    "        Y_last = df_train_2.iloc[-1,:self.p].values.reshape(-1,self.p)\n",
    "        return [y_train,X_train,Y_last[0],sample_weight]\n",
    "    \n",
    "    def update_data(self,endog,exog=None,weight=None,residual=None):\n",
    "        '''\n",
    "        Update the data which allows to use the recent observation to update the AR history to make accurate\n",
    "        prediction for future steps, if we want to avoid estimated value leading to a multi-step prediction\n",
    "        additive error \n",
    "        '''\n",
    "        # Addpending the data\n",
    "        self.endog = np.concatenate((self.endog,np.array(endog)))\n",
    "        \n",
    "        if exog is not None:\n",
    "            self.exog = np.concatenate((self.exog, np.array(exog).reshape(self.exog.shape[0],1)),axis=1)\n",
    "        if weight is not None:\n",
    "            self.w = np.concatenate((self.w,np.array(weight)))\n",
    "        if residual is not None:\n",
    "            self.residual = np.concatenate((self.residual,np.array(residual)))\n",
    "            self.reslast = self.residual[-1:(self.ShQ+self.q+1):-1]\n",
    "        if residual is None:\n",
    "            self.reslast = np.zeros(self.ShQ+self.q)\n",
    "        # This part is helps adding the new observed data and can be used to retrain the model\n",
    "        self.ylast = self.endog[-1:-(self.p+1):-1]\n",
    "        \n",
    "#         #Test Code\n",
    "#         print(\"Update\",self.ylast)\n",
    "#         #End testcode\n",
    "        return\n",
    "    \n",
    "    def update_hist(self,endog,exog=None,weight=None,residual=None):\n",
    "        '''\n",
    "        This code clears the history data used to train/ fit and can be used\n",
    "        when the test and train are disjoint data sets\n",
    "        '''\n",
    "        self.endog = endog\n",
    "        if exog is not None:\n",
    "            self.exog = exog\n",
    "        if weight is not None:\n",
    "            self.w = weight\n",
    "#         if residual in None:\n",
    "#             self.resl\n",
    "        \n",
    "        self.ylast = self.endog[-1:-(self.p+1):-1]\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def WeighedLearn(self,y,X,weight):\n",
    "        '''\n",
    "        This code allows to generate a weighted regression based on the weight and train data\n",
    "        '''\n",
    "        X_sm = sm.add_constant(X)\n",
    "        model = sm.WLS(y,X_sm,weights=weight)\n",
    "        result = model.fit()\n",
    "        return result\n",
    "    \n",
    "    def AR(self):\n",
    "        '''\n",
    "        This code is the AR part of the class.\n",
    "        We first generate the train data by generating p-shifted data train data. We can also add exog but only on\n",
    "        \n",
    "        \n",
    "        \n",
    "        '''\n",
    "        tData = self.getShifted_Dataset()\n",
    "        self.xData = tData[1]\n",
    "        self.yData = tData[0]\n",
    "        self.w = tData[3]\n",
    "#         self.yData = tData[0]\n",
    "#         self.xData = tData[1]\n",
    "        self.ylast = tData[2]\n",
    "#         weight = tData[3]\n",
    "        self.model = self.WeighedLearn(tData[0],tData[1],weight=tData[3])\n",
    "        return self.model,self.ylast,\n",
    "    \n",
    "    def MA(self):\n",
    "        df_res = pd.DataFrame()\n",
    "        df_res['res'] = self.residual\n",
    "        \n",
    "        for i in range(1,self.q+1):\n",
    "            df_res['shifted_%d' % i] = df_res['res'].shift(self.ShQ+i)\n",
    "        df_res['weight'] = self.w\n",
    "        my_data = df_res.dropna()\n",
    "        X_train = my_data.iloc[:,1:self.q+1].values.reshape(-1,self.q)\n",
    "        Y_train = my_data.iloc[:,0].values.reshape(-1,1)\n",
    "        \n",
    "        self.MAmodel = self.WeighedLearn(Y_train,X_train,weight=my_data['weight'].values)\n",
    "        #Get the last residual\n",
    "        self.reslast = my_data.iloc[0:self.ShQ+self.q,0].values.reshape(-1)\n",
    "        return self.MAmodel\n",
    "    \n",
    "    def fit(self):\n",
    "        mymodel,_ = self.AR()\n",
    "        y_pred = np.array(self.model.get_prediction(sm.add_constant(self.xData)).predicted_mean)\n",
    "        df_c = pd.DataFrame()\n",
    "        df_c['pred'] = y_pred\n",
    "        df_c['act'] = self.yData\n",
    "        df_c['res'] = df_c.pred - df_c.act\n",
    "        self.residual = df_c['res'].values\n",
    "#         print(self.residual)\n",
    "        MAModel = self.MA()        \n",
    "        return mymodel,MAModel\n",
    "    \n",
    "    def get_predict(self,exog=None):\n",
    "        if exog is not None:\n",
    "            X_test = [1.0]\n",
    "            X_test.extend(self.ylast)\n",
    "            X_test.extend(exog)\n",
    "            y_pred = self.model.get_prediction(X_test).predicted_mean\n",
    "#             X_rTest = [1.0]\n",
    "#             X_rTest.extend(self.reslast[self.ShQ:self.ShQ+self.q])\n",
    "#             y_MApred = self.MAmodel.get_prediction(X_rTest).predicted_mean\n",
    "#             y_pred =y_pred-y_MApred\n",
    "            return y_pred\n",
    "            \n",
    "           \n",
    "    def get_ARpredict(self,steps=1,exog=None):\n",
    "        # Ensure that the exog and model have the same dimension. Currently we have only one dim of exog\n",
    "        if exog is not None:\n",
    "            #Update\n",
    "            if len(exog) != steps:\n",
    "                print(\"Exog variable incorrect\")\n",
    "                return None\n",
    "        # Assume exog is there\n",
    "        Y_out = []\n",
    "#         print(\"exog\",exog)\n",
    "        if exog is not None:\n",
    "            X_update = []        \n",
    "            X_inp = self.ylast\n",
    "#             print(\"ylast\",self.ylast)\n",
    "            for i in range(steps):\n",
    "                X_test = [1.0]\n",
    "                X_test.extend(X_inp)\n",
    "                X_test.extend(exog[i])\n",
    "#                 print(\"Xtest\",X_test)\n",
    "                y_pred = self.model.get_prediction(X_test).predicted_mean\n",
    "                Y_out.extend(y_pred)\n",
    "                X_i = list(y_pred)\n",
    "                X_i.extend(X_inp[:-1])\n",
    "                X_inp = X_i\n",
    "#                 print(X_inp)\n",
    "            # Predict the data\n",
    "            return Y_out\n",
    "        else:\n",
    "            #No exog\n",
    "            X_inp = self.ylast\n",
    "            for i in range(steps):\n",
    "                X_test = [1.0]\n",
    "                X_test.extend(X_inp)\n",
    "                y_pred = self.model.get_prediction(X_test).predicted_mean\n",
    "                Y_out.extend(y_pred)\n",
    "                X_i = list(y_pred)\n",
    "                X_i.extend(X_inp[:-1])\n",
    "                X_inp = X_i\n",
    "    #             print(X_inp)\n",
    "            return Y_out        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Modifier Code\n",
    "def getDataFrame(fileName):\n",
    "    myDf = pd.read_csv(fileName)\n",
    "    myDf['memLoad'] = myDf['memUse']*100/myDf['memTot']\n",
    "    myDf['memScore'] = myDf['memLoad']*myDf['memfreq']/825000000\n",
    "    myDf['memF'] = myDf['memfreq']*100/825000000\n",
    "    myDf['cpuScore'] = myDf['cpuload']*myDf['cpufreq']/1400000\n",
    "    myDf['cpuF'] = myDf['cpufreq']*100/1000000\n",
    "    return myDf\n",
    "\n",
    "def getNewDataFrame(fileName):\n",
    "    myDf = pd.read_csv(fileName)\n",
    "#     myDf['memLoad'] = myDf['memUse']*100/myDf['memTot']\n",
    "    myDf['mUtil'] = myDf['mUtil'].clip(upper=100)\n",
    "    myDf['mScore'] = myDf['mUtil']*myDf['mFreq']/825000000\n",
    "    myDf['memF'] = myDf['mFreq']*100/825000000\n",
    "    myDf['bcScore'] = myDf['bUtil']*myDf['bFreq']/1400000\n",
    "    myDf['bF'] = myDf['bFreq']*100/1400000\n",
    "    return myDf\n",
    "\n",
    "def getDataMatrix(dx,dy=None): # dy should be multi-dimensional\n",
    "    data_matrix=[]\n",
    "    if dy is None:\n",
    "        for v in dx:\n",
    "            data_matrix.append([round(v,2),round(v,2)])\n",
    "    else:\n",
    "        n_xog = len(dy)\n",
    "        n_len = len(dx)\n",
    "        for i in range(n_len):\n",
    "            v = []\n",
    "            v.append(round(dx[i],2))\n",
    "            for j in range(n_xog):\n",
    "                v.append(round(dy[j][i],2))\n",
    "#         for v in zip(dx,dy):\n",
    "            data_matrix.append(v)\n",
    "    return data_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def butter_filter(data, cutoff, fs=2.0, order=2,passtype='low'):\n",
    "    nyq = 0.5*fs\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    # Get the filter coefficients \n",
    "    b, a = butter(order, normal_cutoff, btype=passtype, analog=False)\n",
    "    y = filtfilt(b, a, data)\n",
    "    y = np.array(y)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latest Code\n",
    "def main(TEST_START=7500,STEP=50,N_CLUST=5,TRAIN_END=7500,ARP=4,PASS='low'):\n",
    "    utilData = getNewDataFrame('./UtilTestFinal')\n",
    "#     print(\"Size of data:\",utilData.head())\n",
    "    df_raw = pd.DataFrame()\n",
    "    df_raw['endog'] = utilData.bUtil\n",
    "    df_raw['exog1'] = utilData.mUtil.diff().shift(1)\n",
    "    df_raw['exog2'] = utilData.mUtil.diff().shift(1)\n",
    "    df_data = df_raw.dropna()\n",
    "    dy = np.clip(butter_filter(df_data.endog.to_numpy(),0.25,passtype=PASS),-100,100)\n",
    "    dz = np.clip(butter_filter(df_data.exog1.to_numpy(),0.25,passtype=PASS),-100,100)\n",
    "    dx = np.clip(butter_filter(df_data.exog2.to_numpy(),0.25,passtype=PASS),-100,100)\n",
    "#     dy=df_data.endog.to_numpy()\n",
    "#     dz=df_data.exog1.to_numpy()\n",
    "#     dx=df_data.exog2.to_numpy()\n",
    "#     plot_pacf(dz,lags=20,alpha=0.05)\n",
    "#     return\n",
    "#     dy = utilData.cpuload.dropna().to_numpy()\n",
    "    nCluster = N_CLUST\n",
    "    y_train = dy[:TRAIN_END]\n",
    "    x_train = dz[:TRAIN_END]\n",
    "    x_train2 = dx[:TRAIN_END]\n",
    "    # Predicting in a different range\n",
    "#     TEST_START = 7500\n",
    "    y_test = dy[TEST_START:]\n",
    "    x_test = dz[TEST_START:]\n",
    "    x_test2 = dx[TEST_START:]\n",
    "    \n",
    "    \n",
    "    y_dmat = getDataMatrix(y_train,[x_train,x_train2])\n",
    "#     y_dmat = getDataMatrix(y_train)\n",
    "    myClust = OSLFuzzyCluster(y_dmat,n_cluster=nCluster)\n",
    "    _, data_weight = myClust.fit()\n",
    "#     print(dy[0])\n",
    "#     print(data_weight)\n",
    "#     print(\"data_cluster\", myClust.get_center())\n",
    "    trans_prob = myClust.get_CondProb()\n",
    "    #Removing Print\n",
    "#     print(trans_prob)\n",
    "#     print(\"\\n\\n\\n\")\n",
    "    # We will send the weights of the data and then train these models\n",
    "    models = []\n",
    "    # Train the models\n",
    "    for i in range(nCluster):\n",
    "        model = OSLARMA(y_train,p=ARP,exog=[x_train,x_train2], weight=data_weight[:,i],q=4,shiftQ=0)\n",
    "        model.fit()\n",
    "#         print(model.summary())\n",
    "        models.append(model)\n",
    "    \n",
    "    \n",
    "    # Error prediction:\n",
    "    RMSE = []\n",
    "    MAE = []\n",
    "    \n",
    "    \n",
    "    # Models are trained now begin prediction\n",
    "#     print(\"last actual value\", y_train[-1],\"Likelihood cluster\", myClust.predict_clust([ y_train[-1], y_train[-1]]))\n",
    "    y_prior = dy[TEST_START-1]\n",
    "    x_prior = dz[TEST_START-1]\n",
    "    x2_prior = dx[TEST_START-1]\n",
    "    Y_ACT=[]\n",
    "    Y_PRED=[]\n",
    "    res_pred = 0\n",
    "    for i in range(STEP):\n",
    "        # Get prob of P(Y(t-1) in i) using cluster\n",
    "        P_prior = myClust.predict_clust([y_prior,x_prior,x2_prior])[0]\n",
    "#         P_prior = myClust.predict_clust([y_prior,y_prior])[0]\n",
    "#         print(\"P\",P_prior)\n",
    "        P_like = []\n",
    "        # Predict likelihood of Y(t) in cluster based on transition prob\n",
    "        for k in range(nCluster):\n",
    "            prob = 0\n",
    "            for j in range(nCluster):\n",
    "                prob += trans_prob[j][k]*P_prior[j]\n",
    "            P_like.append(prob)\n",
    "#         print(\"PLike\",P_like)\n",
    "        \n",
    "#         print(\"PLike\",P_like)\n",
    "        y_act = y_test[i]\n",
    "        y_prior = y_act\n",
    "        x_prior = dz[TEST_START-1+i]\n",
    "        x2_prior = dx[TEST_START-1+i]\n",
    "        y_pred = []\n",
    "        y_est = 0\n",
    "        for j in range(nCluster):\n",
    "            y_pred.append(models[j].get_predict([x_prior,x2_prior])[0])\n",
    "            y_est += y_pred[-1]*P_like[j]\n",
    "#             if P_like[j] > 0.7:\n",
    "#                 y_est = y_pred[-1]\n",
    "#                 break\n",
    "        y_est = max(0, min(y_est-res_pred, 100))\n",
    "        res = y_est - y_act\n",
    "        \n",
    "        if abs(res) < 20:      # removing huge state changes\n",
    "            RMSE.append(res*res)\n",
    "            MAE.append(abs(res))\n",
    "        res_pred = 0.3* res + 0.7*res_pred\n",
    "        res_pred =  max(-20, min(res_pred, 20))\n",
    "#         y_prior = y_est\n",
    "#         print(i, y_act,y_est,x_prior)\n",
    "        Y_ACT.append(y_act)\n",
    "        Y_PRED.append(y_est)\n",
    "        for j in range(nCluster):           \n",
    "            models[j].update_data([y_act],exog=[x_prior,x2_prior],residual=[res])\n",
    "    \n",
    "    print(\"RMSE:\", sqrt(sum(RMSE[ARP:])/len(RMSE[ARP:])), \"  MAE:\", sum(MAE[ARP:])/len(MAE[ARP:]))\n",
    "    df_plot = pd.DataFrame()\n",
    "    df_plot['Actual'] = Y_ACT[ARP:]\n",
    "    df_plot['Predicted'] = Y_PRED[ARP:]\n",
    "    df_plot[['Actual','Predicted']].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes:\n",
    "The estimation ends up in a steady state, and does not update if it observes an mismatch with prediction. Need some sort of nudge, where steady state errors can be handled better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(TRAIN_END=7500,TEST_START=7500,N_CLUST=15,STEP=150,ARP=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(TRAIN_END=7500,TEST_START=7500,N_CLUST=15,STEP=150,ARP=4,PASS='high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_pacf, plot_acf\n",
    "utilData = getNewDataFrame('UtilTestFinal')\n",
    "plot_pacf(utilData['bUtil'].dropna(),lags=20,alpha=0.05,method='ols');\n",
    "# utilData['sUtil'].iloc[600:6250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utilData = getNewDataFrame('UtilTestFinal')\n",
    "# print(utilData.bUtil.diff().shift(1))\n",
    "fig, ax1 = plt.subplots(figsize=(100,10))\n",
    "\n",
    "x = utilData.index.values.tolist()\n",
    "y1 = utilData['bUtil']\n",
    "y2 = utilData['sUtil']\n",
    "y3 = utilData['mUtil']\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "# ax3 = ax2.twinx()\n",
    "\n",
    "ax1.plot(x, y1, 'go')\n",
    "ax2.plot(x, y3, 'bo')\n",
    "# ax3.plot(x, y3, 'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latest AR Code\n",
    "def main(TEST_START=7500,STEP=50,N_CLUST=5,TRAIN_END=6001,ARP=4):\n",
    "    utilData = getNewDataFrame('./UtilTestFinal')\n",
    "    print(\"Size of data:\",utilData.shape)\n",
    "    dy = utilData.bUtil.dropna().to_numpy()\n",
    "    dz = utilData.mUtil.dropna().to_numpy()\n",
    "#     plot_pacf(dz,lags=20,alpha=0.05)\n",
    "#     return\n",
    "#     dy = utilData.cpuload.dropna().to_numpy()\n",
    "    nCluster = N_CLUST\n",
    "    y_train = dy[2:TRAIN_END]\n",
    "    x_train = dz[:TRAIN_END-2]\n",
    "    # Predicting in a different range\n",
    "#     TEST_START = 7500\n",
    "    y_test = dy[TEST_START:]\n",
    "    x_test = dz[TEST_START-2:]\n",
    "    \n",
    "    \n",
    "    y_dmat = getDataMatrix(y_train,x_train)\n",
    "    myClust = OSLFuzzyCluster(y_dmat,n_cluster=nCluster)\n",
    "    _, data_weight = myClust.fit()\n",
    "#     print(dy[0])\n",
    "#     print(data_weight)\n",
    "#     print(\"data_cluster\", myClust.get_center())\n",
    "    trans_prob = myClust.get_CondProb()\n",
    "    #Removing Print\n",
    "#     print(trans_prob)\n",
    "#     print(\"\\n\\n\\n\")\n",
    "    # We will send the weights of the data and then train these models\n",
    "    models = []\n",
    "    # Train the models\n",
    "    for i in range(nCluster):\n",
    "        model = OSLARMA(y_train,p=ARP,exog=x_train, weight=data_weight[:,i])\n",
    "        model.AR()\n",
    "#         print(model.summary())\n",
    "        models.append(model)\n",
    "    \n",
    "    # Models are trained now begin prediction\n",
    "#     print(\"last actual value\", y_train[-1],\"Likelihood cluster\", myClust.predict_clust([ y_train[-1], y_train[-1]]))\n",
    "    y_prior = dy[TEST_START-1]\n",
    "    x_prior = dz[TEST_START-3]\n",
    "    Y_ACT=[]\n",
    "    Y_PRED=[]\n",
    "    for i in range(STEP):\n",
    "        # Get prob of P(Y(t-1) in i) using cluster\n",
    "        P_prior = myClust.predict_clust([y_prior,x_prior])[0]\n",
    "#         print(\"P\",P_prior)\n",
    "        P_like = []\n",
    "        # Predict likelihood of Y(t) in cluster based on transition prob\n",
    "        for k in range(nCluster):\n",
    "            prob = 0\n",
    "            for j in range(nCluster):\n",
    "                prob += trans_prob[j][k]*P_prior[j]\n",
    "            P_like.append(prob)\n",
    "        \n",
    "#         print(\"PLike\",P_like)\n",
    "        y_act = y_test[i]\n",
    "        y_prior = y_act\n",
    "        x_prior = dz[TEST_START-2+i]\n",
    "        y_pred = []\n",
    "        y_est = 0\n",
    "        for j in range(nCluster):\n",
    "            y_pred.append(models[j].get_ARpredict(1,[x_prior])[0])\n",
    "            y_est += y_pred[-1]*P_like[j]\n",
    "        \n",
    "#         print(i, y_act,y_est,x_prior)\n",
    "        Y_ACT.append(y_act)\n",
    "        Y_PRED.append(y_est)\n",
    "        for j in range(nCluster):\n",
    "            models[j].update_data([y_act],[x_prior])\n",
    "    df_plot = pd.DataFrame()\n",
    "    df_plot['Actual'] = Y_ACT\n",
    "    df_plot['Predicted'] = Y_PRED\n",
    "    df_plot[['Actual','Predicted']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old Code\n",
    "# Testing  \n",
    "def main():\n",
    "    utilData = getDataFrame('./Util_Run1.csv')\n",
    "    dy = utilData.cpuload.dropna().to_numpy()\n",
    "    dz = utilData.memLoad.dropna().to_numpy()\n",
    "#     dy = utilData.cpuload.dropna().to_numpy()\n",
    "    nCluster = 5\n",
    "    y_train = dy[:2000]\n",
    "    x_train = dz [:2000]\n",
    "    y_test = dy[2000:]\n",
    "    x_test = dz[2000:]\n",
    "    y_dmat = getDataMatrix(y_train,x_train)\n",
    "    myClust = OSLFuzzyCluster(y_dmat,n_cluster=nCluster)\n",
    "    _, data_weight = myClust.fit()\n",
    "#     print(dy[0])\n",
    "#     print(data_weight)\n",
    "#     print(\"data_cluster\", myClust.get_center())\n",
    "    trans_prob = myClust.get_CondProb()\n",
    "    print(trans_prob)\n",
    "    print(\"\\n\\n\\n\")\n",
    "    # We will send the weights of the data and then train these models\n",
    "    models = []\n",
    "    # Train the models\n",
    "    for i in range(nCluster):\n",
    "        model = OSLARMA(y_train,p=4,exog=x_train, weight=data_weight[:,i])\n",
    "        model.fit()\n",
    "#         print(model.summary())\n",
    "        models.append(model)\n",
    "    \n",
    "    # Models are trained now begin prediction\n",
    "#     print(\"last actual value\", y_train[-1],\"Likelihood cluster\", myClust.predict_clust([ y_train[-1], y_train[-1]]))\n",
    "    y_prior = y_train[-1]\n",
    "    x_prior = x_train[-1]\n",
    "    for i in range(50):\n",
    "        # Get prob of P(Y(t-1) in i) using cluster\n",
    "        P_prior = myClust.predict_clust([y_prior,x_prior])[0]\n",
    "#         print(\"P\",P_prior)\n",
    "        P_like = []\n",
    "        # Predict likelihood of Y(t) in cluster based on transition prob\n",
    "        for k in range(nCluster):\n",
    "            prob = 0\n",
    "            for j in range(nCluster):\n",
    "                prob += trans_prob[j][k]*P_prior[j]\n",
    "            P_like.append(prob)\n",
    "        \n",
    "#         print(\"PLike\",P_like)\n",
    "        y_act = y_test[i]\n",
    "        y_prior = y_act\n",
    "        x_prior = x_test[i]\n",
    "        y_pred = []\n",
    "        y_est = 0\n",
    "        for j in range(nCluster):\n",
    "            y_pred.append(models[j].get_predict(1,[x_prior])[0])\n",
    "            y_est += y_pred[-1]*P_like[j]\n",
    "        \n",
    "        print(i, y_act,y_est,x_prior)\n",
    "        \n",
    "        for j in range(nCluster):\n",
    "            models[j].update_data([y_act],[x_prior])\n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(TRAIN_END=7500,TEST_START=7500,N_CLUST=5,STEP=50,ARP=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
