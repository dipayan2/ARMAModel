{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "# from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.graphics.tsaplots import plot_predict\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.rcParams['figure.figsize'] = (10, 5)\n",
    "# plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for data extraction\n",
    "1. In this section we will load the raw data and extract utilization and frequency value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2674, 6)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utilData = pd.read_csv('./Util_Run1.csv')\n",
    "utilData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>memUse</th>\n",
       "      <th>memTot</th>\n",
       "      <th>memfreq</th>\n",
       "      <th>cpuid</th>\n",
       "      <th>cpuload</th>\n",
       "      <th>cpufreq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>604872</td>\n",
       "      <td>4612865</td>\n",
       "      <td>165000000</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>675</td>\n",
       "      <td>28712</td>\n",
       "      <td>275000000</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1191888</td>\n",
       "      <td>12624777</td>\n",
       "      <td>413000000</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>773492</td>\n",
       "      <td>8196989</td>\n",
       "      <td>275000000</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1144</td>\n",
       "      <td>57890</td>\n",
       "      <td>206000000</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>300000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    memUse    memTot    memfreq  cpuid  cpuload  cpufreq\n",
       "0   604872   4612865  165000000      0       31   600000\n",
       "1      675     28712  275000000      0        4   200000\n",
       "2  1191888  12624777  413000000      0        8   200000\n",
       "3   773492   8196989  275000000      0        9   300000\n",
       "4     1144     57890  206000000      0        7   300000"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utilData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "utilData['memLoad'] = utilData['memUse']*100/utilData['memTot']\n",
    "utilData['memScore'] = utilData['memLoad']*utilData['memfreq']/825000000\n",
    "utilData['memF'] = utilData['memfreq']*100/825000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "utilData['cpuScore'] = utilData['cpuload']*utilData['cpufreq']/1400000\n",
    "utilData['cpuF'] = utilData['cpufreq']*100/1000000\n",
    "utilData['memShift'] = utilData['memLoad'].shift()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utilData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_train = utilData.iloc[0:2136]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1 = utilData.iloc[0:534]\n",
    "train_2 = utilData.iloc[534:1068]\n",
    "train_3 = utilData.iloc[1068:1602]\n",
    "train_4 = utilData.iloc[1602:2136]\n",
    "test_data = utilData.iloc[2136:]\n",
    "print(train_1.tail())\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Space to experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling \n",
    "In this section we will model the data and test its accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utilData['memShift'] = utilData['memLoad'].shift()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARIMA(tot_train[['cpuF']], order=(3,2,1), exog=tot_train[['memLoad','cpuScore']])\n",
    "result = model.fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = ARIMA(train_1['cpuF'], order=(2,0,0))\n",
    "result1 = model1.fit()\n",
    "print(result1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = ARIMA(train_2['cpuF'], order=(2,0,1), exog=train_2['cpuScore'])\n",
    "result2 = model2.fit()\n",
    "print(result2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = ARIMA(train_3['cpuF'], order=(2,0,1), exog=train_3['cpuScore'])\n",
    "result3 = model3.fit()\n",
    "print(result3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = ARIMA(train_4['cpuF'], order=(4,0,1), exog=train_4['cpuScore'])\n",
    "result4 = model4.fit()\n",
    "print(result4.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Accuracy\n",
    "We will run both the models to test its accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to test the accuracy of prediction\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = result.forecast(steps=538,exog=test_data[['memScore','cpuScore']])\n",
    "forecast = forecast.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtest_data = test_data.iloc[0:20]\n",
    "pforecast = result.get_forecast(steps=20,exog=mtest_data[['memScore','cpuScore']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDf = pforecast.summary_frame(alpha=0.05)\n",
    "myDf['memScore'] = mtest_data['memScore'].values\n",
    "myDf['cpuScore'] = mtest_data['cpuScore'].values\n",
    "myDf['Actual freq'] = mtest_data['cpuF'].values\n",
    "myDf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtest_data = test_data.iloc[300:320]\n",
    "pforecast = result.get_forecast(steps=20,exog=mtest_data[['memScore','cpuScore']])\n",
    "myDf = pforecast.summary_frame(alpha=0.05)\n",
    "myDf['memScore'] = mtest_data['memScore'].values\n",
    "myDf['cpuScore'] = mtest_data['cpuScore'].values\n",
    "myDf['Actual freq'] = mtest_data['cpuF'].values\n",
    "myDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast1 = result1.forecast(steps=538)\n",
    "forecast1 = forecast1.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast2 = result2.forecast(steps=538,exog=test_data['cpuScore'])\n",
    "forecast2 = forecast2.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast3 = result3.forecast(steps=538,exog=test_data['cpuScore'])\n",
    "forecast3 = forecast3.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast4 = result4.forecast(steps=538,exog=test_data['cpuScore'])\n",
    "forecast4 = forecast4.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test_data['cpuF'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RMSE', np.sqrt(metrics.mean_squared_error(y_true,forecast)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RMSE_1', np.sqrt(metrics.mean_squared_error(y_true,forecast1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RMSE_2', np.sqrt(metrics.mean_squared_error(y_true,forecast2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RMSE_3', np.sqrt(metrics.mean_squared_error(y_true,forecast3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RMSE_4', np.sqrt(metrics.mean_squared_error(y_true,forecast4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the time series\n",
    "1. We will split the time series in fragments of 25 entries and then create 4 different entries and merge to train the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strain_1 = utilData.iloc[0:25]\n",
    "strain_2 = utilData.iloc[25:50]\n",
    "strain_3 = utilData.iloc[50:75]\n",
    "strain_4 = utilData.iloc[75:100]\n",
    "stest_data = utilData.iloc[2200:]\n",
    "stest_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strain_1.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create our data here\n",
    "for i in range(100,2136,100):\n",
    "    i_1,i_2,i_3,i_4 = i,i+25,i+50,i+75\n",
    "    i_end = i+100\n",
    "    strain_1 = strain_1.append(utilData.iloc[i_1:i_2],ignore_index=True)\n",
    "    strain_2 = strain_2.append(utilData.iloc[i_2:i_3],ignore_index=True)\n",
    "    strain_3 = strain_3.append(utilData.iloc[i_3:i_4],ignore_index=True)\n",
    "    strain_4 = strain_4.append(utilData.iloc[i_4:i_end],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strain_4.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling the split time series\n",
    "We are using the newly generated split dataset to train and verify its acuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smodel1 = ARIMA(strain_1['cpuScore'], order=(4,0,3), exog=strain_1['cpuF'])\n",
    "sresult1 = smodel1.fit()\n",
    "print(sresult1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smodel2 = ARIMA(strain_2['cpuF'], order=(4,0,1))\n",
    "sresult2 = smodel2.fit()\n",
    "print(sresult2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smodel3 = ARIMA(strain_3['cpuF'], order=(4,0,1), exog=strain_3['cpuScore'])\n",
    "sresult3 = smodel3.fit()\n",
    "print(sresult3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smodel4 = ARIMA(strain_4['cpuF'], order=(4,0,1), exog=strain_4['cpuScore'])\n",
    "sresult4 = smodel4.fit()\n",
    "print(sresult4.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Phase for split\n",
    "We are calculating the RMSE of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sforecast1 = sresult1.forecast(steps=474,exog=stest_data['cpuScore'])\n",
    "sforecast1 = sforecast1.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sy_true = stest_data['cpuF'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RMSE_1', np.sqrt(metrics.mean_squared_error(sy_true,sforecast1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sforecast2 = sresult2.forecast(steps=474)\n",
    "sforecast2 = sforecast2.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RMSE_2', np.sqrt(metrics.mean_squared_error(sy_true,sforecast2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sforecast3 = sresult3.forecast(steps=474,exog=stest_data['cpuScore'])\n",
    "sforecast3 = sforecast3.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RMSE_3', np.sqrt(metrics.mean_squared_error(sy_true,sforecast3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sforecast4 = sresult4.forecast(steps=474,exog=stest_data['cpuScore'])\n",
    "sforecast4 = sforecast4.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RMSE_4', np.sqrt(metrics.mean_squared_error(sy_true,sforecast4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note; the results are better for smaller prediction interval. \n",
    "Does it mean we need to retrain every small interval. (We did 100 v. 474)\n",
    "That is not an effective prediction model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping the data by a column\n",
    "We will now model for each state of the cpufrequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the unique cpuF values\n",
    "cpuFArr = tot_train['cpuF'].unique()\n",
    "cpuFArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(utilData['memfreq'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtrain = tot_train.groupby(['cpuF'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtrain.get_group(cpuFArr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapTrain = {}\n",
    "for  val in cpuFArr:\n",
    "    mapTrain[val] = gtrain.get_group(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gresult ={}\n",
    "for val in cpuFArr:\n",
    "    print(\"\\n\\nVal\",val)\n",
    "    gmodel = ARIMA(mapTrain[val]['cpuload'], order=(1,0,1))\n",
    "    gresult[val] = gmodel.fit()\n",
    "    print(gresult[val].summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The results are very confusing\n",
    "What should serve as the endogenous and exogenous variable for this model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the data\n",
    "Understand how does the state looks like with the change\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utilData.plot(y='cpuload')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utilData.plot(y='memLoad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis= utilData.index.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(50,2))\n",
    "fig, ax1 = plt.subplots(figsize=(50, 10))\n",
    "\n",
    "x = utilData.index.values.tolist()\n",
    "y1 = utilData['cpuF']\n",
    "y2 = utilData['memF']\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax1.plot(x, y1, 'go')\n",
    "ax2.plot(x, y2, 'bo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(50,10))\n",
    "fig, ax1 = plt.subplots(figsize=(50, 10))\n",
    "\n",
    "x = utilData[1800:2000].index.values.tolist()\n",
    "y1 = utilData['cpuScore'].iloc[1800:2000]\n",
    "y2 = utilData['memScore'].iloc[1800:2000]\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax1.plot(x, y1, 'go')\n",
    "ax2.plot(x, y2, 'bo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(100,40))\n",
    "fig, ax1 = plt.subplots(figsize=(100,10))\n",
    "\n",
    "x = utilData.index.values.tolist()\n",
    "y1 = utilData['cpuScore']\n",
    "y2 = utilData['cpuF']\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax1.plot(x, y1, 'go')\n",
    "ax2.plot(x, y2, 'bo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(80,10))\n",
    "fig, ax1 = plt.subplots(figsize=(80,10))\n",
    "\n",
    "x = utilData.index.values.tolist()\n",
    "y1 = utilData['memScore']\n",
    "y2 = utilData['memF']\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax1.plot(x, y1, 'go')\n",
    "ax2.plot(x, y2, 'bo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(50,2))\n",
    "fig, ax1 = plt.subplots(figsize=(50,10))\n",
    "\n",
    "x = utilData.index.values.tolist()\n",
    "y1 = utilData['cpuScore']\n",
    "y2 = utilData['memF']\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax1.plot(x, y1, 'go')\n",
    "ax2.plot(x, y2, 'bo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty space for plotting more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Characterizing state and load behaviour\n",
    "How does cpuload, memload, and their frequencies fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diffCorr(x,y):\n",
    "    print(\"Pearson\",stats.pearsonr(x,y))\n",
    "    print(\"Spearman\",stats.spearmanr(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = utilData['cpuF'].to_numpy()\n",
    "x2 = utilData['memfreq'].to_numpy()\n",
    "diffCorr(x1,x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = utilData['memLoad'].to_numpy()\n",
    "x2 = utilData['cpuF'].to_numpy()\n",
    "diffCorr(x1,x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = utilData['cpuF'].to_numpy()\n",
    "x2 = utilData['cpuScore'].to_numpy()\n",
    "diffCorr(x1,x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = utilData['memScore'][0:500].to_numpy()\n",
    "x2 = utilData['cpuScore'][0:500].to_numpy()\n",
    "diffCorr(x1,x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = utilData['memfreq'].to_numpy()\n",
    "x2 = utilData['memLoad'].to_numpy()\n",
    "diffCorr(x1,x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = utilData['cpuF'].to_numpy()\n",
    "x2 = utilData['memLoad'].to_numpy()\n",
    "diffCorr(x1,x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = utilData['memfreq'].to_numpy()\n",
    "x2 = utilData['cpuScore'].to_numpy()\n",
    "diffCorr(x1,x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = utilData['memF'][500:1000].to_numpy()\n",
    "x2 = utilData['cpuScore'][500:1000].to_numpy()\n",
    "diffCorr(x1,x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: Memory has more cross resource predictive power but cpu is better at its own prediction\n",
    "We will utilize these values and try run a linear model to check how the coefficients look for a multi-dimensional ARMA model and a linear/other regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Empty space for code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting PACF and ACF\n",
    "We need to find out the level of autocorrelation in the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_pacf, plot_acf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pacf(tot_train['cpuScore'].diff().dropna(), lags=20, alpha=0.05,method='ols');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf(tot_train['cpuload'].diff().dropna(), lags=20, alpha=0.05);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pacf(tot_train['memLoad'].diff().dropna(), lags=40, alpha=0.05,method='ols');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf(tot_train['memF'].diff().dropna(), lags=50, alpha=0.05);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Space for futher analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding cross value lagged correlation\n",
    "Using xcorr to get the idea of lagged dependency among variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = utilData['cpuF'].to_numpy()\n",
    "x2 = utilData['memF'].to_numpy()\n",
    "plt.xcorr(x1,x2,normed=True,maxlags=200);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = utilData['cpuF'].to_numpy()\n",
    "x2 = utilData['cpuScore'].to_numpy()\n",
    "plt.xcorr(x1,x2,normed=True,maxlags=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = utilData['cpuScore'].diff().dropna().to_numpy()\n",
    "x2 = utilData['memScore'].diff().dropna().to_numpy()\n",
    "plt.xcorr(x1,x2,normed=True,usevlines=True,maxlags=200);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = utilData['memF'].to_numpy()\n",
    "x2 = utilData['memScore'].to_numpy()\n",
    "plt.xcorr(x1,x2,normed=True,maxlags=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = utilData['memF'].to_numpy()\n",
    "x2 = utilData['cpuScore'].to_numpy()\n",
    "plt.xcorr(x1,x2,normed=True,usevlines=True,maxlags=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from statsmodels.graphics.tsaplots import plot_acf\n",
    "# fig, (ax1, ax2, ax3) = plt.subplots(3)\n",
    "# plot_acf(utilData.cpuScore, ax=ax1,lags=50, alpha=0.05);\n",
    "plot_acf(utilData.cpuload.diff().dropna(),lags=50, alpha=0.05);\n",
    "# plot_acf(utilData.cpuload.diff().diff().dropna(),lags=50, alpha=0.05);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stationarity Check\n",
    "We check whether a given variable is stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "result = adfuller(utilData['cpuload'].diff().dropna())\n",
    "print('ADF Statistic: %f' % result[0])\n",
    "print('p-value: %f' % result[1])\n",
    "print('Critical Values:')\n",
    "for key, value in result[4].items():\n",
    "    print('\\t%s: %.3f' % (key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Space for further checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustered Model approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "## model\n",
    "##\n",
    "nr_sample = 2000\n",
    "train = utilData.iloc[0:nr_sample]\n",
    "test  = utilData.iloc[nr_sample:]\n",
    "\n",
    "nr_chunk_sample = 50            # number of samples in a chunk\n",
    "nr_chunk = int(nr_sample / nr_chunk_sample) # number of chunks\n",
    "chunk_model = []                # which model chunk_model[i] belongs to\n",
    "\n",
    "nr_model = 7                    # number of models\n",
    "nr_model_switch_padding = 20    # number of padding samples between model switch\n",
    "models = []\n",
    "\n",
    "nr_kmean_epoch = 60             # number of epoch in k-mean\n",
    "pr_kmean_update = 0.5           # learning rate of k-mean\n",
    "\n",
    "#model_order = (2, 0, [2])\n",
    "model_order = (3, 1, 1)\n",
    "endog_key = 'cpuload'\n",
    "exog_key = 'cpuF'\n",
    "\n",
    "\n",
    "def diff(series):   # difference operation\n",
    "    res = []\n",
    "    for t in range(len(series)-1):\n",
    "        if np.isnan(series[t]) or np.isnan(series[t+1]):\n",
    "            res.append(0)\n",
    "        else:\n",
    "            res.append(series[t+1] - series[t])\n",
    "    return res\n",
    "\n",
    "\n",
    "def accu(series):   # accumulation operation\n",
    "    res = []\n",
    "    tsum = 0\n",
    "    for t in (range(len(series))):\n",
    "        tsum = tsum + series[t]\n",
    "        res.append(tsum)\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_sample(m):  # get sample for model m\n",
    "    y, x = [], []\n",
    "    for i in range(nr_chunk):\n",
    "        # copy sample\n",
    "        if chunk_model[i] != m:\n",
    "            continue\n",
    "        j = i + 1   # find the last contiguous mode\n",
    "        while j < nr_chunk and chunk_model[j] == m:\n",
    "            j = j + 1\n",
    "        for k in range(i*nr_chunk_sample, j*nr_chunk_sample):\n",
    "            y.append(train.loc[k, endog_key]) \n",
    "            x.append(train.loc[k, exog_key ])\n",
    "\n",
    "        # padding\n",
    "        if j + 1 >= nr_chunk:\n",
    "            continue\n",
    "        for k in range(nr_model_switch_padding):\n",
    "            y.append(np.nan)\n",
    "            x.append(np.nan)\n",
    "            #x.append(x[-1]) # find a better value \n",
    "    return y, x\n",
    "\n",
    "\n",
    "def fit_models():\n",
    "    for m in range(nr_model):\n",
    "        y, x = get_sample(m)\n",
    "        y = diff(y) # length of y will by decreated by 1\n",
    "        #x.pop()     # to mathch the length of x and y\n",
    "        x = diff(x)\n",
    "        if len(y) < nr_chunk_sample:\n",
    "            continue\n",
    "        #model = ARIMA(y, order=model_order).fit()\n",
    "        model = ARIMA(y, order=model_order, exog=x).fit()\n",
    "        models.append(model)\n",
    "        print('mse', model.mse)\n",
    "\n",
    "\n",
    "def init_chunks():\n",
    "    for m in range(nr_chunk):\n",
    "        chunk_model.append(random.randrange(nr_model))\n",
    "\n",
    "\n",
    "def update_chunks():\n",
    "    for c in range(nr_chunk):\n",
    "        if random.random() > pr_kmean_update: # update slowly\n",
    "            continue\n",
    "\n",
    "        mses = []\n",
    "        for m in range(nr_model):\n",
    "            start, end = c*nr_chunk_sample, (c+1)*nr_chunk_sample\n",
    "            y = list(train[endog_key][start:end])\n",
    "            x = list(train[exog_key ][start:end])\n",
    "            y = diff(y) # length of y will by decreated by 1\n",
    "            #x.pop()     # to mathch the length of x and y\n",
    "            x = diff(x)\n",
    "            #res = models[m].apply(y, refit=False)\n",
    "            res = models[m].apply(y, exog=x, refit=False)\n",
    "            mses.append(res.mse)\n",
    "        best = np.argmin(mses)\n",
    "        chunk_model[c] = best\n",
    "\n",
    "\n",
    "def estimate_models():\n",
    "    # k-mean\n",
    "    random.seed(1)\n",
    "    init_chunks()\n",
    "    for i in range(nr_kmean_epoch):\n",
    "        print('[', i, '/', nr_kmean_epoch, ']--------------')\n",
    "        fit_models()\n",
    "        update_chunks()\n",
    "\n",
    "def summary_models():\n",
    "    for i in range(nr_model):\n",
    "        print(\"Model Num: \", i)\n",
    "        print(models[i].summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate_models()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPU Load prediction: Clustered approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Fuzzy C mean clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skfda\n",
    "from skfda.ml.clustering import FuzzyCMeans, KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_Cluster = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "dy = utilData.cpuload.diff().dropna().to_numpy()\n",
    "dz = utilData.memLoad.diff().dropna().to_numpy()\n",
    "dx = utilData.cpuScore.diff().dropna().to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2673,), (2673,))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dz.shape,dx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(dz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_point = [0,1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_matrix = []\n",
    "for v in zip(dy,dz,dx):\n",
    "    data_matrix.append([round(v[0],2),round(v[0],2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_matrix\n",
    "data_matrix1 = []\n",
    "for v in zip(dy,dz,dx):\n",
    "    data_matrix1.append([round(v[0],1),round(v[1],2),round(v[2],2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-27.  , -10.76, -12.71],\n",
       "       [  4.  ,   7.09,   0.57],\n",
       "       [  1.  ,  -0.  ,   0.79],\n",
       "       ...,\n",
       "       [ -3.  ,  -4.29,  -0.64],\n",
       "       [  1.  ,   0.  ,   0.21],\n",
       "       [  2.  ,   0.66,   0.43]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(data_matrix1)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzy_kmeans = FuzzyCMeans(n_clusters=N_Cluster,n_init=5 ,random_state=0,fuzzifier=2,max_iter=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = skfda.FDataGrid(data_matrix,grid_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzy_kmeans.fit(fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd1 = skfda.FDataGrid(data_matrix1,grid_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fuzzy_kmeans.fit(fd1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzy_kmeans.cluster_centers_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzy_kmeans1 = FuzzyCMeans(n_clusters=N_Cluster,n_init=5 ,init=None,fuzzifier=2,max_iter=400,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>FuzzyCMeans(max_iter=400, n_init=5)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">FuzzyCMeans</label><div class=\"sk-toggleable__content\"><pre>FuzzyCMeans(max_iter=400, n_init=5)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "FuzzyCMeans(max_iter=400, n_init=5)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzzy_kmeans1.fit(fd1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 3.20071656],\n",
       "        [-0.57896637],\n",
       "        [ 2.02066826]],\n",
       "\n",
       "       [[-3.11929999],\n",
       "        [ 0.55046571],\n",
       "        [-1.96219104]]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzzy_kmeans1.cluster_centers_.data_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzy_kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = fuzzy_kmeans.predict(fd)\n",
    "mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilData.iloc[201:221].cpufreq\n",
    "fd[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_count=fuzzy_kmeans.predict_proba(fd) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Conditional Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate transition\n",
    "Count = [0 for _ in range(N_Cluster)]\n",
    "TP_count = [[0 for _ in range(N_Cluster)] for _ in range(N_Cluster)]\n",
    "for i in range(len(mem)):\n",
    "    Count[mem[i]] += 1\n",
    "    if i>0:\n",
    "        TP_count[mem[i-1]][mem[i]] += 1\n",
    "\n",
    "TP_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cond_Prob = [[0 for _ in range(N_Cluster)] for _ in range(N_Cluster)]\n",
    "for i in range(N_Cluster):\n",
    "    for j in range(N_Cluster):\n",
    "        Cond_Prob[i][j] = round(TP_count[i][j]/Count[j],3)\n",
    "Cond_Prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Caclulate fuzzy transition\n",
    "PCount = [0 for _ in range(N_Cluster)]\n",
    "PTP_count = [[0 for _ in range(N_Cluster)] for _ in range(N_Cluster)]\n",
    "for i in range(len(prob_count)):\n",
    "    for c in range(N_Cluster):\n",
    "        PCount[c] += prob_count[i][c]\n",
    "        if i>0:\n",
    "            for cn in range(N_Cluster):\n",
    "                PTP_count[cn][c] += prob_count[i-1][cn]*prob_count[i][c]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCond_Prob = [[0 for _ in range(N_Cluster)] for _ in range(N_Cluster)]\n",
    "for i in range(N_Cluster):\n",
    "    for j in range(N_Cluster):\n",
    "        PCond_Prob[i][j] = round(PTP_count[i][j]/PCount[j],3)\n",
    "PCond_Prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = list(mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(N_Cluster):\n",
    "    print(i,\" --> \", mem.count(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(200,10))\n",
    "\n",
    "x = list(range(len(mem)))\n",
    "y1 = mem\n",
    "y2 = utilData['cpuF'].iloc[1:]\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax1.plot(x, y1, 'go')\n",
    "ax2.plot(x, y2, 'bo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Architecture\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing the complete steps for the analysis and desing of our model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the clustering model\n",
    "import skfda\n",
    "from skfda.ml.clustering import FuzzyCMeans, KMeans\n",
    "# Num_Cluster = 4\n",
    "# # Data for clustering\n",
    "# dCpu = utilData.cpuload.diff().dropna().to_numpy()\n",
    "# dMem = utilData.memLoad.diff().dropna().to_numpy()\n",
    "# DataGrid for clustering\n",
    "\n",
    "def getFDataGrid(inp):\n",
    "    inp = np.array(inp)\n",
    "    n_dim = inp.ndim\n",
    "    if n_dim == 1:\n",
    "        grid_point = list(range(len(inp)))\n",
    "        data_grid = []\n",
    "        data_grid.append(inp)\n",
    "        fd = skfda.FDataGrid(data_grid,grid_point)\n",
    "        return fd\n",
    "    elif n_dim == 2:\n",
    "        grid_point = list(range(len(inp[0])))\n",
    "        data_grid = []\n",
    "        for i in range(len(inp)):\n",
    "            data_grid.append(inp[i])\n",
    "        \n",
    "        fd = skfda.FDataGrid(data_grid,grid_point)\n",
    "        return fd\n",
    "    else:\n",
    "        raise Exception(\"Cannot handle more than 2 dimension for now\") \n",
    "        return None\n",
    "    \n",
    "def getClusterModel(data,n_cluster=5,init_cluster=None):    \n",
    "    fD = getFDataGrid(data)\n",
    "    if fD == None:\n",
    "        print(\"No data to cluster\")\n",
    "        return None\n",
    "    fuzzyModel = FuzzyCMeans(n_clusters=n_cluster,n_init=5 ,init=init_cluster,fuzzifier=2,max_iter=400,random_state=0)\n",
    "    fuzzyModel.fit(fD)\n",
    "    return fuzzyModel\n",
    "    \n",
    "def get_C_prediction(model,data):\n",
    "    fd = getFDataGrid(data)\n",
    "    return model.predict_proba(fd)\n",
    "    \n",
    "def get_CondProb(model,data):\n",
    "    prob_count = get_C_prediction(model,data)\n",
    "    n_cluster = prob_count.shape[1]\n",
    "    \n",
    "    PCount = [0 for _ in range(n_cluster)]\n",
    "    PTP_count = [[0 for _ in range(n_cluster)] for _ in range(n_cluster)]\n",
    "    for i in range(len(prob_count)):\n",
    "        for c in range(n_cluster):\n",
    "            PCount[c] += prob_count[i][c]\n",
    "            if i>0:\n",
    "                for cn in range(n_cluster):\n",
    "                    PTP_count[cn][c] += prob_count[i-1][cn]*prob_count[i][c]\n",
    "    \n",
    "    \n",
    "    PCond_Prob = [[0 for _ in range(n_cluster)] for _ in range(n_cluster)]\n",
    "    for i in range(n_cluster):\n",
    "        for j in range(n_cluster):\n",
    "            PCond_Prob[i][j] = round(PTP_count[i][j]/PCount[j],3)\n",
    "    return PCond_Prob\n",
    "\n",
    "def getShifted_Dataset(endog,p=1,exog=None,weight=None , shiftedExog=None):\n",
    "    # Currently accepting only one dimensional exog if any\n",
    "    if p < 1:\n",
    "        print('Error needs atleast 1')\n",
    "        return None\n",
    "    plen = p\n",
    "    df_temp = pd.DataFrame()\n",
    "    df_temp['endog'] = endog\n",
    "    # Getting shifted dataset\n",
    "    for i in range(1,p+1):\n",
    "        df_temp['Shift_endog_%d' % i] = df_temp['endog'].shift(i)\n",
    "    if exog is not None:\n",
    "        df_temp['exog'] = exog\n",
    "        plen += 1\n",
    "    if shiftedExog is not None:\n",
    "        for i in range(1,shiftedExog+1):\n",
    "            df_temp['Shift_exog_%d' % i] = df_temp['exog'].shift(i)\n",
    "    if weight is not None:\n",
    "        df_temp['Weight'] = weight\n",
    "    elif weight is None:\n",
    "        df_temp['Weight'] = np.ones(endog.shape[0])\n",
    "        \n",
    "    df_train_2 = df_temp.dropna()\n",
    "    X_train = df_train_2.iloc[:,1:plen+1].values.reshape(-1,plen)\n",
    "    y_train = df_train_2.iloc[:,0].values.reshape(-1)\n",
    "    sample_weight = df_train_2['Weight'].values\n",
    "    X_last = df_train_2.iloc[-1,:plen].values.reshape(-1,plen)\n",
    "    return [y_train,X_train,X_last[0],sample_weight]\n",
    "\n",
    "def WeighedLearn(y,X,weight):\n",
    "    import statsmodels.api as sm\n",
    "    X_sm = sm.add_constant(X)\n",
    "    model = sm.WLS(y,X_sm,weights=weight)\n",
    "    result = model.fit()\n",
    "    return result\n",
    "\n",
    "def ARPredict(model,y_last,steps=1,exog=None):\n",
    "    # Ensure that the exog and model have the same dimension. Currently we have only one dim of exog\n",
    "    if exog is not None:\n",
    "        #Update\n",
    "        if len(exog) != steps:\n",
    "            print(\"Input variable incorrect\")\n",
    "            return None\n",
    "    # Assume exog is there\n",
    "    Y_out = []\n",
    "    if exog is not None:\n",
    "        X_update = []        \n",
    "        X_inp = y_last\n",
    "        for i in range(steps):\n",
    "            X_test = [1.0]\n",
    "#             X_test.extend(X_inp)\n",
    "            X_test.append(exog[i])\n",
    "            y_pred = model.get_prediction(X_test).predicted_mean\n",
    "            Y_out.extend(y_pred)\n",
    "            X_inp = y_pred+X_inp[:-1]\n",
    "            print(X_inp)\n",
    "        # Predict the data\n",
    "        return Y_out\n",
    "    else:\n",
    "        #No exog\n",
    "        X_inp = y_last\n",
    "        for i in range(steps):\n",
    "            X_test = [1.0]\n",
    "            X_test.extend(X_inp)\n",
    "            y_pred = model.get_prediction(X_test).predicted_mean\n",
    "            Y_out.extend(y_pred)\n",
    "            X_i = list(y_pred)\n",
    "            X_i.extend(X_inp[:-1])\n",
    "            X_inp = X_i\n",
    "#             print(X_inp)\n",
    "        return Y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNewDataFrame(fileName):\n",
    "    myDf = pd.read_csv(fileName)\n",
    "#     myDf['memLoad'] = myDf['memUse']*100/myDf['memTot']\n",
    "    myDf['mUtil'] = myDf['mUtil'].clip(upper=100)\n",
    "    myDf['mScore'] = myDf['mUtil']*myDf['mFreq']/825000000\n",
    "    myDf['memF'] = myDf['mFreq']*100/825000000\n",
    "    myDf['bcScore'] = myDf['bUtil']*myDf['bFreq']/1400000\n",
    "#     myDf['cpuF'] = myDf['cpufreq']*100/1000000\n",
    "    return myDf\n",
    "\n",
    "utilData = getNewDataFrame('./UtilTestFinal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8560,) (8560,) (8560,)\n"
     ]
    }
   ],
   "source": [
    "#Testing new modification \n",
    "# 1. Download the data\n",
    "mEndog = utilData.bUtil.to_numpy()\n",
    "mExog = utilData.mUtil.to_numpy()\n",
    "mDexog = utilData.mUtil.diff().to_numpy()\n",
    "print(mEndog.shape,mExog.shape,mDexog.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tD = getShifted_Dataset(mEndog,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "def WeighedLearn(y,X,weight):\n",
    "#     import statsmodels.api as sm\n",
    "    X_sm = sm.add_constant(X)\n",
    "    model = sm.WLS(y,X_sm,weights=weight)\n",
    "    result = model.fit()\n",
    "    return result\n",
    "\n",
    "model = WeighedLearn(tD[0],tD[1],weight=tD[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = sm.add_constant(tD[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_act = tD[0]\n",
    "y_pred = model.get_prediction(X_test).predicted_mean\n",
    "df_c = pd.DataFrame()\n",
    "df_c['predict'] = y_pred\n",
    "df_c['act'] = y_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c['res'] = df_c.act - df_c.predict\n",
    "myAr = df_c['res'].values\n",
    "# myAr.shape\n",
    "plot_pacf(myAr, lags=30, alpha=0.05,method='ols');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MA(q,res):\n",
    "    df_res = pd.DataFrame()\n",
    "    df_res['res'] = res\n",
    "#     print(df_res)\n",
    "    for i in range(1,q+1):\n",
    "        df_res['Shifted_%d' % i] = df_res['res'].shift(5+i)\n",
    "    \n",
    "#     print(df_res)\n",
    "    my_data = df_res.dropna()\n",
    "    my_train = pd.DataFrame(my_data[0:6000])\n",
    "    my_test = pd.DataFrame(my_data[6000:])\n",
    "#     print(my_train)\n",
    "    X_train = my_train.iloc[:,1:].values.reshape(-1,q)\n",
    "    X_sm = sm.add_constant(X_train)\n",
    "#     print(X_train)\n",
    "    Y_train = my_train.iloc[:,0].values.reshape(-1,1)\n",
    "#     print(X_sm)\n",
    "#     weight = np.ones(Y_train.shape[0])\n",
    "\n",
    "    model = sm.OLS(Y_train,X_sm)\n",
    "    result = model.fit()\n",
    "    X_test = my_test.iloc[:,1:].values.reshape(-1,q)\n",
    "    X_f = sm.add_constant(X_test)\n",
    "    Y_f = my_test.iloc[:,0].values.reshape(-1)\n",
    "    y_pred = result.get_prediction(X_f).predicted_mean\n",
    "#     print(y_pred)\n",
    "    df_plot = pd.DataFrame()\n",
    "    df_plot['res'] = Y_f\n",
    "    df_plot['pred'] = y_pred\n",
    "    df_plot[['res','pred']].plot()\n",
    "    return result\n",
    "\n",
    "mymodel = MA(5,myAr)\n",
    "\n",
    "\n",
    "# Get the \n",
    "mymodel.summary()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c[['res']].plot(kind='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = pd.DataFrame()\n",
    "df_plot['Actual'] = y_act[2000:2200]\n",
    "df_plot['Predicted'] = y_pred[2000:2200]\n",
    "df_plot[['Actual','Predicted']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skfda\n",
    "from skfda.ml.clustering import FuzzyCMeans, KMeans\n",
    "class FuzzyCluster:\n",
    "    def __init__(self,data,n_cluster=3,init_cluster=None,fuzzifier=2):\n",
    "        self.data = data\n",
    "        self.nclust = n_cluster\n",
    "        self.init_clust = init_cluster\n",
    "        self.model = None\n",
    "        self.predictClus = None\n",
    "        self.fData = None\n",
    "        self.fuzzifier=fuzzifier\n",
    "        self.probCond = None\n",
    "    \n",
    "    def getFDataGrid(self, inp):\n",
    "        inp = np.array(inp)\n",
    "        n_dim = inp.ndim\n",
    "        if n_dim == 1:\n",
    "            grid_point = list(range(len(inp)))\n",
    "            data_grid = []\n",
    "            data_grid.append(inp)\n",
    "            fd = skfda.FDataGrid(data_grid,grid_point)\n",
    "            return fd\n",
    "        elif n_dim == 2:\n",
    "            grid_point = list(range(len(inp[0])))\n",
    "            data_grid = []\n",
    "            for i in range(len(inp)):\n",
    "                data_grid.append(inp[i])\n",
    "\n",
    "            fd = skfda.FDataGrid(data_grid,grid_point)\n",
    "            return fd\n",
    "        else:\n",
    "            raise Exception(\"Cannot handle more than 2 dimension for now\") \n",
    "            return None\n",
    "    \n",
    "    def update_data(self,data):\n",
    "        self.data.extend(data)\n",
    "        return\n",
    "        \n",
    "        \n",
    "    def fit(self):    \n",
    "        self.fData = self.getFDataGrid(self.data)\n",
    "        if self.fData == None:\n",
    "            print(\"No data to cluster\")\n",
    "            return None\n",
    "        self.model = FuzzyCMeans(n_clusters=self.nclust,n_init=5 ,init=self.init_clust,fuzzifier=self.fuzzifier,max_iter=400,random_state=0)\n",
    "        self.model.fit(self.fData)\n",
    "        self.predictClus = self.model.predict_proba(self.fData)\n",
    "        return self.model,self.predictClus\n",
    "    \n",
    "    def predict_clust(self,data):\n",
    "        fd = self.getFDataGrid(data)\n",
    "        return self.model.predict_proba(fd)\n",
    "    \n",
    "    def get_center(self):\n",
    "        return self.model.cluster_centers_.data_matrix\n",
    "        \n",
    "    def get_CondProb(self):\n",
    "        prob_count = self.predictClus\n",
    "        n_cluster = self.nclust\n",
    "\n",
    "        PCount = [0 for _ in range(n_cluster)]\n",
    "        PTP_count = [[0 for _ in range(n_cluster)] for _ in range(n_cluster)]\n",
    "        for i in range(len(prob_count)):\n",
    "            for c in range(n_cluster):\n",
    "                PCount[c] += prob_count[i][c]\n",
    "                if i>0:\n",
    "                    for cn in range(n_cluster):\n",
    "                        PTP_count[cn][c] += prob_count[i-1][cn]*prob_count[i][c]\n",
    "\n",
    "\n",
    "        self.probCond = [[0 for _ in range(n_cluster)] for _ in range(n_cluster)]\n",
    "        for i in range(n_cluster):\n",
    "            for j in range(n_cluster):\n",
    "                self.probCond[i][j] = round(PTP_count[i][j]/PCount[j],3)\n",
    "        return self.probCond\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MyClust = FuzzyCluster(data_matrix,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MyClust.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MyClust.predict_clust([-2,-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MyClust.get_CondProb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dCpu = utilData.bUtil.diff().dropna().to_numpy()\n",
    "dMem = utilData.memLoad.diff().dropna().to_numpy()\n",
    "# Create a combined Data\n",
    "dM = []\n",
    "for v in zip(dCpu,dMem):\n",
    "    dM.append([round(v[0],1),round(v[1],2)])\n",
    "myModel = getClusterModel(dM)\n",
    "# get_C_prediction(myModel,dM)[:,0]\n",
    "dCpu[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Out = getShifted_Dataset(dCpu,5)\n",
    "Model = WeighedLearn(Out[0],Out[1],weight=Out[3])\n",
    "ARPredict(Model,Out[2],5)\n",
    "# Model.summary()\n",
    "print(Out[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dCpu[-1:-6:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_verify = ARIMA(mEndog[:2500], order=(5,0,0),exog=dMem[:2500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_verify = model_verify.fit()\n",
    "print(result_verify.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_verify.forecast(steps=4,exog=dMem[2500:2504])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARMA:\n",
    "    def __init__(self,endog,p=3,q=2,exog=None,weight=None):\n",
    "        self.endog = endog\n",
    "        self.p = p\n",
    "        self.q = q\n",
    "        self.exog = exog\n",
    "        self.w = weight\n",
    "        self.model = None\n",
    "        self.MAmodel = None\n",
    "        self.xData = None\n",
    "        self.yData = None\n",
    "        self.ylast = None\n",
    "        self.fitData = None\n",
    "        self.residual = None\n",
    "        self.reslast = None\n",
    "        \n",
    "    def getShifted_Dataset(self):\n",
    "        # Currently accepting only one dimensional exog if any\n",
    "        if self.p < 1:\n",
    "            print('Error needs atleast 1')\n",
    "            return None\n",
    "        plen = self.p\n",
    "        df_temp = pd.DataFrame()\n",
    "        df_temp['endog'] = self.endog\n",
    "        # Getting shifted dataset\n",
    "        for i in range(1,self.p+1):\n",
    "            df_temp['Shift_endog_%d' % i] = df_temp['endog'].shift(i)\n",
    "        if self.exog is not None:\n",
    "            # We are assuming that the xvalues are 2D value\n",
    "            self.exog = np.array(self.exog)\n",
    "            for xid in range(self.exog.shape[0]):\n",
    "                df_temp['exog_%d' % xid] = self.exog[xid]\n",
    "                plen += 1\n",
    "        if self.w is not None:\n",
    "            df_temp['Weight'] = self.w\n",
    "        elif self.w is None:\n",
    "            df_temp['Weight'] = np.ones(self.endog.shape[0])\n",
    "\n",
    "        df_train_2 = df_temp.dropna()\n",
    "        X_train = df_train_2.iloc[:,1:plen+1].values.reshape(-1,plen)\n",
    "        y_train = df_train_2.iloc[:,0].values.reshape(-1,1)\n",
    "        sample_weight = df_train_2['Weight'].values\n",
    "        X_last = df_train_2.iloc[-1,:self.p].values.reshape(-1,self.p)\n",
    "        return [y_train,X_train,X_last[0],sample_weight]\n",
    "    \n",
    "    def update_data(self,endog,exog=None,weight=None):\n",
    "        # Addpending the data\n",
    "        self.endog.extend(endog)\n",
    "        if exog is not None:\n",
    "            self.exog.extend(exog)\n",
    "        if weight is not None:\n",
    "            self.w.exend(weight)\n",
    "        # This part is helps adding the new observed data and can be used to retrain the model\n",
    "        self.ylast = self.endog[-1:-(self.p+1):-1]\n",
    "        return\n",
    "    \n",
    "    def WeighedLearn(self,y,X,weight):\n",
    "        import statsmodels.api as sm\n",
    "        X_sm = sm.add_constant(X)\n",
    "        model = sm.WLS(y,X_sm,weights=weight)\n",
    "        result = model.fit()\n",
    "        return result\n",
    "    \n",
    "    def AR(self):\n",
    "        tData = self.getShifted_Dataset()\n",
    "#         self.yData = tData[0]\n",
    "#         self.xData = tData[1]\n",
    "        self.ylast = tData[2]\n",
    "#         weight = tData[3]\n",
    "        self.model = self.WeighedLearn(tData[0],tData[1],weight=tData[3])\n",
    "        # Get the residuals:\n",
    "        return self.model,self.ylast\n",
    "    \n",
    "    def MA(self):\n",
    "        df_res = pd.DataFrame()\n",
    "        df_res['res'] = res\n",
    "        #     print(df_res)\n",
    "        for i in range(1,q+1):\n",
    "            df_res['Shifted_%d' % i] = df_res['res'].shift(i)\n",
    "\n",
    "        #     print(df_res)\n",
    "        my_data = df_res.dropna()\n",
    "        my_train = pd.DataFrame(my_data[0:6000])\n",
    "        my_test = pd.DataFrame(my_data[6000:])\n",
    "        #     print(my_train)\n",
    "        X_train = my_train.iloc[:,1:].values.reshape(-1,q)\n",
    "        X_sm = sm.add_constant(X_train)\n",
    "        #     print(X_train)\n",
    "        Y_train = my_train.iloc[:,0].values.reshape(-1,1)\n",
    "        #     print(X_sm)\n",
    "        #     weight = np.ones(Y_train.shape[0])\n",
    "\n",
    "        model = sm.OLS(Y_train,X_sm)\n",
    "        result = model.fit()\n",
    "        X_test = my_test.iloc[:,1:].values.reshape(-1,q)\n",
    "        X_f = sm.add_constant(X_test)\n",
    "        Y_f = my_test.iloc[:,0].values.reshape(-1)\n",
    "        y_pred = result.get_prediction(X_f).predicted_mean\n",
    "        #     print(y_pred)\n",
    "        df_plot = pd.DataFrame()\n",
    "        df_plot['res'] = Y_f\n",
    "        df_plot['pred'] = y_pred\n",
    "        df_plot[['res','pred']].plot()\n",
    "        return result\n",
    "           \n",
    "    def get_predict(self,steps=1,exog=None):\n",
    "        # Ensure that the exog and model have the same dimension. Currently we have only one dim of exog\n",
    "        if exog is not None:\n",
    "            #Update\n",
    "            if len(exog) != steps:\n",
    "                print(\"Exog variable incorrect\")\n",
    "                return None\n",
    "        # Assume exog is there\n",
    "        Y_out = []\n",
    "#         print(\"exog\",exog)\n",
    "        if exog is not None:\n",
    "            X_update = []        \n",
    "            X_inp = self.ylast\n",
    "#             print(\"ylast\",self.ylast)\n",
    "            for i in range(steps):\n",
    "                X_test = [1.0]\n",
    "                X_test.extend(X_inp)\n",
    "                X_test.extend(exog[i])\n",
    "#                 print(\"Xtest\",X_test)\n",
    "                y_pred = self.model.get_prediction(X_test).predicted_mean\n",
    "                Y_out.extend(y_pred)\n",
    "                X_i = list(y_pred)\n",
    "                X_i.extend(X_inp[:-1])\n",
    "                X_inp = X_i\n",
    "#                 print(X_inp)\n",
    "            # Predict the data\n",
    "            return Y_out\n",
    "        else:\n",
    "            #No exog\n",
    "            X_inp = self.ylast\n",
    "            for i in range(steps):\n",
    "                X_test = [1.0]\n",
    "                X_test.extend(X_inp)\n",
    "                y_pred = self.model.get_prediction(X_test).predicted_mean\n",
    "                Y_out.extend(y_pred)\n",
    "                X_i = list(y_pred)\n",
    "                X_i.extend(X_inp[:-1])\n",
    "                X_inp = X_i\n",
    "    #             print(X_inp)\n",
    "            return Y_out        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<statsmodels.regression.linear_model.RegressionResultsWrapper at 0x7fc6a00487f0>,\n",
       " array([27.,  4., 45.,  7., 35.]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight = np.linspace(0.5,1.0,2500)\n",
    "classTest = ARMA(mEndog[:2500],p=5,exog=[mExog[:2500],mDexog[:2500]],weight=weight)\n",
    "classTest.AR()\n",
    "# classTest.get_predict(steps=3,exog=dMem[2500:2503])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mDexog[2500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[23.25678888961429]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classTest.get_predict(steps=1,exog=[[mExog[2500],mDexog[2500]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dMem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [1,2,3]\n",
    "Y = [4,5]\n",
    "X.extend(Y)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array([[1,2,3],[3,2,3]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modular Code\n",
    "Here we will use the classes to actually create the pipeline for our estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skfda\n",
    "from skfda.ml.clustering import FuzzyCMeans, KMeans\n",
    "class OSLFuzzyCluster:\n",
    "    def __init__(self,data,n_cluster=3,init_cluster=None,fuzzifier=2):\n",
    "        self.data = data\n",
    "        self.nclust = n_cluster\n",
    "        self.init_clust = init_cluster\n",
    "        self.model = None\n",
    "        self.predictClus = None\n",
    "        self.fData = None\n",
    "        self.fuzzifier=fuzzifier\n",
    "        self.probCond = None\n",
    "    \n",
    "    def getFDataGrid(self, inp):\n",
    "        inp = np.array(inp)\n",
    "        n_dim = inp.ndim\n",
    "        if n_dim == 1:\n",
    "            grid_point = list(range(len(inp)))\n",
    "            data_grid = []\n",
    "            data_grid.append(inp)\n",
    "            fd = skfda.FDataGrid(data_grid,grid_point)\n",
    "            return fd\n",
    "        elif n_dim == 2:\n",
    "            grid_point = list(range(len(inp[0])))\n",
    "            data_grid = []\n",
    "            for i in range(len(inp)):\n",
    "                data_grid.append(inp[i])\n",
    "\n",
    "            fd = skfda.FDataGrid(data_grid,grid_point)\n",
    "            return fd\n",
    "        else:\n",
    "            raise Exception(\"Cannot handle more than 2 dimension for now\") \n",
    "            return None\n",
    "    \n",
    "    def update_data(self,data):\n",
    "        self.data = np.append(self.data,np.array(data))\n",
    "        return\n",
    "        \n",
    "        \n",
    "    def fit(self):    \n",
    "        self.fData = self.getFDataGrid(self.data)\n",
    "        if self.fData == None:\n",
    "            print(\"No data to cluster\")\n",
    "            return None\n",
    "        self.model = FuzzyCMeans(n_clusters=self.nclust,n_init=5 ,init=self.init_clust,fuzzifier=self.fuzzifier,max_iter=400,random_state=0)\n",
    "        self.model.fit(self.fData)\n",
    "        self.predictClus = self.model.predict_proba(self.fData)\n",
    "        return self.model,self.predictClus\n",
    "    \n",
    "    def predict_clust(self,data):\n",
    "        fd = self.getFDataGrid(data)\n",
    "        return self.model.predict_proba(fd)\n",
    "    \n",
    "    def get_center(self):\n",
    "        return self.model.cluster_centers_.data_matrix\n",
    "        \n",
    "    def get_CondProb(self):\n",
    "        prob_count = self.predictClus\n",
    "        n_cluster = self.nclust\n",
    "\n",
    "        PCount = [0 for _ in range(n_cluster)]\n",
    "        PTP_count = [[0 for _ in range(n_cluster)] for _ in range(n_cluster)]\n",
    "        for i in range(len(prob_count)):\n",
    "            for c in range(n_cluster):\n",
    "                PCount[c] += prob_count[i][c]\n",
    "                if i>0:\n",
    "                    for cn in range(n_cluster):\n",
    "                        PTP_count[cn][c] += prob_count[i-1][cn]*prob_count[i][c]\n",
    "\n",
    "\n",
    "        self.probCond = [[0 for _ in range(n_cluster)] for _ in range(n_cluster)]\n",
    "        for i in range(n_cluster):\n",
    "            for j in range(n_cluster):\n",
    "                self.probCond[i][j] = round(PTP_count[i][j]/PCount[i],3) # P(Y(t) in j / Y(t-1) in i)\n",
    "        return self.probCond\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OSLARMA:\n",
    "    def __init__(self,endog,p=3,exog=None,weight=None):\n",
    "        self.endog = endog\n",
    "        self.p = p\n",
    "        self.exog = exog\n",
    "        self.w = weight\n",
    "        self.model = None\n",
    "        self.xData = None\n",
    "        self.yData = None\n",
    "        self.ylast = None\n",
    "        self.fitData = None\n",
    "    \n",
    "    def summary(self):\n",
    "        return self.model.summary()\n",
    "        \n",
    "    def getShifted_Dataset(self):\n",
    "        # Currently accepting only one dimensional exog if any\n",
    "        if self.p < 1:\n",
    "            print('Error needs atleast 1')\n",
    "            return None\n",
    "        plen = self.p\n",
    "        df_temp = pd.DataFrame()\n",
    "        df_temp['endog'] = self.endog\n",
    "        # Getting shifted dataset\n",
    "        for i in range(1,self.p+1):\n",
    "            df_temp['Shift_endog_%d' % i] = df_temp['endog'].shift(i)\n",
    "        if self.exog is not None:\n",
    "            df_temp['exog'] = self.exog\n",
    "            plen += 1\n",
    "        if self.w is not None:\n",
    "            df_temp['Weight'] = self.w\n",
    "        elif self.w is None:\n",
    "            df_temp['Weight'] = np.ones(self.endog.shape[0])\n",
    "\n",
    "        df_train_2 = df_temp.dropna()\n",
    "        X_train = df_train_2.iloc[:,1:plen+1].values.reshape(-1,plen)\n",
    "        y_train = df_train_2.iloc[:,0].values.reshape(-1,1)\n",
    "        sample_weight = df_train_2['Weight'].values\n",
    "        X_last = df_train_2.iloc[-1,:self.p].values.reshape(-1,self.p)\n",
    "        return [y_train,X_train,X_last[0],sample_weight]\n",
    "    \n",
    "    def update_data(self,endog,exog=None,weight=None):\n",
    "        # Addpending the data\n",
    "        self.endog = np.concatenate((self.endog,np.array(endog)))\n",
    "        \n",
    "        if exog is not None:\n",
    "            self.exog = np.concatenate((self.exog, np.array(exog)))\n",
    "        if weight is not None:\n",
    "            self.w = np.concatenate((self.w,np.array(weight)))\n",
    "        # This part is helps adding the new observed data and can be used to retrain the model\n",
    "        self.ylast = self.endog[-1:-(self.p+1):-1]\n",
    "#         #Test Code\n",
    "#         print(\"Update\",self.ylast)\n",
    "#         #End testcode\n",
    "        return\n",
    "    \n",
    "    def WeighedLearn(self,y,X,weight):\n",
    "        import statsmodels.api as sm\n",
    "        X_sm = sm.add_constant(X)\n",
    "        model = sm.WLS(y,X_sm,weights=weight)\n",
    "        result = model.fit()\n",
    "        return result\n",
    "    \n",
    "    def fit(self):\n",
    "        tData = self.getShifted_Dataset()\n",
    "#         self.yData = tData[0]\n",
    "#         self.xData = tData[1]\n",
    "        self.ylast = tData[2]\n",
    "#         weight = tData[3]\n",
    "        self.model = self.WeighedLearn(tData[0],tData[1],weight=tData[3])\n",
    "        return self.model,self.ylast\n",
    "           \n",
    "    def get_predict(self,steps=1,exog=None):\n",
    "        # Ensure that the exog and model have the same dimension. Currently we have only one dim of exog\n",
    "        if exog is not None:\n",
    "            #Update\n",
    "            if len(exog) != steps:\n",
    "                print(\"Exog variable incorrect\")\n",
    "                return None\n",
    "        # Assume exog is there\n",
    "        Y_out = []\n",
    "#         print(\"exog\",exog)\n",
    "        if exog is not None:\n",
    "            X_update = []        \n",
    "            X_inp = self.ylast\n",
    "#             print(\"ylast\",self.ylast)\n",
    "            for i in range(steps):\n",
    "                X_test = [1.0]\n",
    "                X_test.extend(X_inp)\n",
    "                X_test.append(exog[i])\n",
    "#                 print(\"Xtest\",X_test)\n",
    "                y_pred = self.model.get_prediction(X_test).predicted_mean\n",
    "                Y_out.extend(y_pred)\n",
    "                X_i = list(y_pred)\n",
    "                X_i.extend(X_inp[:-1])\n",
    "                X_inp = X_i\n",
    "#                 print(X_inp)\n",
    "            # Predict the data\n",
    "            return Y_out\n",
    "        else:\n",
    "            #No exog\n",
    "            X_inp = self.ylast\n",
    "            for i in range(steps):\n",
    "                X_test = [1.0]\n",
    "                X_test.extend(X_inp)\n",
    "                y_pred = self.model.get_prediction(X_test).predicted_mean\n",
    "                Y_out.extend(y_pred)\n",
    "                X_i = list(y_pred)\n",
    "                X_i.extend(X_inp[:-1])\n",
    "                X_inp = X_i\n",
    "    #             print(X_inp)\n",
    "            return Y_out        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataFrame(fileName):\n",
    "    myDf = pd.read_csv(fileName)\n",
    "    myDf['memLoad'] = myDf['memUse']*100/myDf['memTot']\n",
    "    myDf['memScore'] = myDf['memLoad']*myDf['memfreq']/825000000\n",
    "    myDf['memF'] = myDf['memfreq']*100/825000000\n",
    "    myDf['cpuScore'] = myDf['cpuload']*myDf['cpufreq']/1400000\n",
    "    myDf['cpuF'] = myDf['cpufreq']*100/1000000\n",
    "    return myDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataMatrix(dx,dy=None):\n",
    "    data_matrix=[]\n",
    "    if dy is None:\n",
    "        for v in dx:\n",
    "            data_matrix.append([round(v,2),round(v,2)])\n",
    "    else:\n",
    "        for v in zip(dx,dy):\n",
    "            data_matrix.append([round(v[0],2),round(v[1],2)])\n",
    "    return data_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing without exog\n",
    "def main():\n",
    "    utilData = getDataFrame('./Util_Run1.csv')\n",
    "#     dy = utilData.cpuload.diff().dropna().to_numpy()\n",
    "    dy = utilData.memLoad.dropna().to_numpy()\n",
    "    nCluster = 5\n",
    "    y_train = dy[:2000]\n",
    "    y_test = dy[2000:]\n",
    "    y_dmat = getDataMatrix(y_train)\n",
    "    myClust = OSLFuzzyCluster(y_dmat,n_cluster=nCluster)\n",
    "    _, data_weight = myClust.fit()\n",
    "#     print(dy[0])\n",
    "#     print(data_weight)\n",
    "#     print(\"data_cluster\", myClust.get_center())\n",
    "    trans_prob = myClust.get_CondProb()\n",
    "    print(trans_prob)\n",
    "    print(\"\\n\\n\\n\")\n",
    "    # We will send the weights of the data and then train these models\n",
    "    models = []\n",
    "    # Train the models\n",
    "    for i in range(nCluster):\n",
    "        model = OSLARMA(y_train,p=5,weight=data_weight[:,i])\n",
    "        model.fit()\n",
    "#         print(model.summary())\n",
    "        models.append(model)\n",
    "    \n",
    "    # Models are trained now begin prediction\n",
    "#     print(\"last actual value\", y_train[-1],\"Likelihood cluster\", myClust.predict_clust([ y_train[-1], y_train[-1]]))\n",
    "    y_prior = y_train[-1]\n",
    "    for i in range(100):\n",
    "        # Get prob of P(Y(t-1) in i) using cluster\n",
    "        P_prior = myClust.predict_clust([y_prior,y_prior])[0]\n",
    "#         print(\"P\",P_prior)\n",
    "        P_like = []\n",
    "        # Predict likelihood of Y(t) in cluster based on transition prob\n",
    "        for k in range(nCluster):\n",
    "            prob = 0\n",
    "            for j in range(nCluster):\n",
    "                prob += trans_prob[j][k]*P_prior[j]\n",
    "            P_like.append(prob)\n",
    "        \n",
    "#         print(\"PLike\",P_like)\n",
    "        y_act = y_test[i]\n",
    "        y_prior = y_act\n",
    "        y_pred = []\n",
    "        y_est = 0\n",
    "        for j in range(nCluster):\n",
    "            y_pred.append(models[j].get_predict()[0])\n",
    "            y_est += y_pred[-1]*P_like[j]\n",
    "        \n",
    "        print(i, y_act,y_est)\n",
    "        \n",
    "        for j in range(nCluster):\n",
    "            models[j].update_data([y_act])\n",
    "            \n",
    "\n",
    "main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3])\n",
    "b= np.array([3,2,1])\n",
    "c = b-a\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.zeros(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [[1,3,5],[2,4,6]]\n",
    "X = np.array(X)\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.concatenate((X,np.array([[7,8]]).reshape(X.shape[0],1)),axis=1)\n",
    "X.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
